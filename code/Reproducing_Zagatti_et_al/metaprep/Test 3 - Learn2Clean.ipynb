{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('csv_results/tic-tac-toe.data.csv')\n",
    "\n",
    "import sklearn.model_selection\n",
    "\n",
    "train, test = sklearn.model_selection.train_test_split(df, random_state=1)\n",
    "\n",
    "train.to_csv('csv_results/tictactoe_train.csv', index = False, header=True)\n",
    "test.to_csv('csv_results/tictactoe_test.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('csv_results/cardio.csv', sep=';')\n",
    "\n",
    "import sklearn.model_selection\n",
    "\n",
    "train, test = sklearn.model_selection.train_test_split(df, random_state=1)\n",
    "\n",
    "train.to_csv('csv_results/cardio_train.csv', index = False, header=True)\n",
    "test.to_csv('csv_results/cardio_test.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('csv_results/bike_buyers_clean.csv')\n",
    "\n",
    "import sklearn.model_selection\n",
    "\n",
    "train, test = sklearn.model_selection.train_test_split(df, random_state=1)\n",
    "\n",
    "train.to_csv('csv_results/bikebuyers_train.csv', index = False, header=True)\n",
    "test.to_csv('csv_results/bikebuyers_test.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Learn2Clean\n",
      "Learn2Clean - Pipeline construction -- CPU time: 0.2393198013305664 seconds\n",
      "=== Start Pipeline Execution ===\n",
      "\n",
      "\n",
      "Strategy# 0 : Greedy traversal for starting state MICE\n",
      "MICE -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      ">>Imputation \n",
      "* For train dataset\n",
      "Before imputation:\n",
      "Total 866 missing values in ['Age', 'Cabin', 'Embarked']\n",
      "- 177 numerical missing values in ['Age']\n",
      "- 689 non-numerical missing values in ['Cabin', 'Embarked']\n",
      "After imputation:\n",
      "Total 689 missing values\n",
      "- 0 numerical missing values\n",
      "- 689 non-numerical missing values\n",
      "* For test dataset\n",
      "Before imputation:\n",
      "Total 1280 missing values in ['Age', 'Cabin', 'Embarked', 'Fare']\n",
      "- 264 numerical missing values in ['Age', 'Fare']\n",
      "- 1016 non-numerical missing values in ['Cabin', 'Embarked']\n",
      "After imputation:\n",
      "Total 1016 missing values\n",
      "- 0 numerical missing values\n",
      "- 1016 non-numerical missing values\n",
      "Imputation done -- CPU time: 0.10902023315429688 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7047940074906367\n",
      "\n",
      "Classification done -- CPU time: 21.983202695846558 seconds\n",
      "End Pipeline CPU time: 22.10233974456787 seconds\n",
      "\n",
      "\n",
      "Strategy# 1 : Greedy traversal for starting state EM\n",
      "EM -> MM -> IQR -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      ">>Imputation \n",
      "* For train dataset\n",
      "Before imputation:\n",
      "Total 866 missing values in ['Age', 'Cabin', 'Embarked']\n",
      "- 177 numerical missing values in ['Age']\n",
      "- 689 non-numerical missing values in ['Cabin', 'Embarked']\n",
      "After imputation:\n",
      "Total 689 missing values\n",
      "- 0 numerical missing values\n",
      "- 689 non-numerical missing values\n",
      "* For test dataset\n",
      "Before imputation:\n",
      "Total 1280 missing values in ['Age', 'Cabin', 'Embarked', 'Fare']\n",
      "- 264 numerical missing values in ['Age', 'Fare']\n",
      "- 1016 non-numerical missing values in ['Cabin', 'Embarked']\n",
      "After imputation:\n",
      "Total 1016 missing values\n",
      "- 0 numerical missing values\n",
      "- 1016 non-numerical missing values\n",
      "Imputation done -- CPU time: 0.3969392776489258 seconds\n",
      "\n",
      ">>Normalization \n",
      "* For train dataset\n",
      "... train dataset\n",
      "* For test dataset\n",
      "... test dataset\n",
      "Normalization done -- CPU time: 0.0252988338470459 seconds\n",
      "\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "1 outlying rows have been removed\n",
      "* For test dataset\n",
      "0 outlying rows have been removed\n",
      "Outlier detection and removal done -- CPU time: 0.022569894790649414 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7000000000000001\n",
      "\n",
      "Classification done -- CPU time: 21.884411334991455 seconds\n",
      "End Pipeline CPU time: 22.343098640441895 seconds\n",
      "\n",
      "\n",
      "Strategy# 2 : Greedy traversal for starting state KNN\n",
      "KNN -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      ">>Imputation \n",
      "* For train dataset\n",
      "Before imputation:\n",
      "Total 866 missing values in ['Age', 'Cabin', 'Embarked']\n",
      "- 177 numerical missing values in ['Age']\n",
      "- 689 non-numerical missing values in ['Cabin', 'Embarked']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/fernandozagatti/anaconda3/lib/python3.7/site-packages/fancyimpute/solver.py:58: UserWarning: Input matrix is not missing any values\n",
      "  warnings.warn(\"Input matrix is not missing any values\")\n",
      "/home/fernandozagatti/anaconda3/lib/python3.7/site-packages/fancyimpute/solver.py:58: UserWarning: Input matrix is not missing any values\n",
      "  warnings.warn(\"Input matrix is not missing any values\")\n",
      "/home/fernandozagatti/anaconda3/lib/python3.7/site-packages/fancyimpute/solver.py:58: UserWarning: Input matrix is not missing any values\n",
      "  warnings.warn(\"Input matrix is not missing any values\")\n",
      "/home/fernandozagatti/anaconda3/lib/python3.7/site-packages/fancyimpute/solver.py:58: UserWarning: Input matrix is not missing any values\n",
      "  warnings.warn(\"Input matrix is not missing any values\")\n",
      "/home/fernandozagatti/anaconda3/lib/python3.7/site-packages/fancyimpute/solver.py:58: UserWarning: Input matrix is not missing any values\n",
      "  warnings.warn(\"Input matrix is not missing any values\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After imputation:\n",
      "Total 689 missing values\n",
      "- 0 numerical missing values\n",
      "- 689 non-numerical missing values\n",
      "* For test dataset\n",
      "Before imputation:\n",
      "Total 1280 missing values in ['Age', 'Cabin', 'Embarked', 'Fare']\n",
      "- 264 numerical missing values in ['Age', 'Fare']\n",
      "- 1016 non-numerical missing values in ['Cabin', 'Embarked']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fernandozagatti/anaconda3/lib/python3.7/site-packages/fancyimpute/solver.py:58: UserWarning: Input matrix is not missing any values\n",
      "  warnings.warn(\"Input matrix is not missing any values\")\n",
      "/home/fernandozagatti/anaconda3/lib/python3.7/site-packages/fancyimpute/solver.py:58: UserWarning: Input matrix is not missing any values\n",
      "  warnings.warn(\"Input matrix is not missing any values\")\n",
      "/home/fernandozagatti/anaconda3/lib/python3.7/site-packages/fancyimpute/solver.py:58: UserWarning: Input matrix is not missing any values\n",
      "  warnings.warn(\"Input matrix is not missing any values\")\n",
      "/home/fernandozagatti/anaconda3/lib/python3.7/site-packages/fancyimpute/solver.py:58: UserWarning: Input matrix is not missing any values\n",
      "  warnings.warn(\"Input matrix is not missing any values\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After imputation:\n",
      "Total 1016 missing values\n",
      "- 0 numerical missing values\n",
      "- 1016 non-numerical missing values\n",
      "Imputation done -- CPU time: 3.135084629058838 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.6363795255930087\n",
      "\n",
      "Classification done -- CPU time: 22.30101752281189 seconds\n",
      "End Pipeline CPU time: 25.44336462020874 seconds\n",
      "\n",
      "\n",
      "Strategy# 3 : Greedy traversal for starting state MF\n",
      "MF -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      ">>Imputation \n",
      "* For train dataset\n",
      "Before imputation:\n",
      "Total 866 missing values in ['Age', 'Cabin', 'Embarked']\n",
      "- 177 numerical missing values in ['Age']\n",
      "- 689 non-numerical missing values in ['Cabin', 'Embarked']\n",
      "After imputation:\n",
      "Total 0 missing values\n",
      "- 0 numerical missing values\n",
      "- 0 non-numerical missing values\n",
      "* For test dataset\n",
      "Before imputation:\n",
      "Total 1280 missing values in ['Age', 'Cabin', 'Embarked', 'Fare']\n",
      "- 264 numerical missing values in ['Age', 'Fare']\n",
      "- 1016 non-numerical missing values in ['Cabin', 'Embarked']\n",
      "After imputation:\n",
      "Total 0 missing values\n",
      "- 0 numerical missing values\n",
      "- 0 non-numerical missing values\n",
      "Imputation done -- CPU time: 0.08135008811950684 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7047940074906367\n",
      "\n",
      "Classification done -- CPU time: 23.069905042648315 seconds\n",
      "End Pipeline CPU time: 23.158997774124146 seconds\n",
      "\n",
      "\n",
      "Strategy# 4 : Greedy traversal for starting state DS\n",
      "DS -> IQR -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      ">>Normalization \n",
      "* For train dataset\n",
      "... train dataset\n",
      "* For test dataset\n",
      "... test dataset\n",
      "Normalization done -- CPU time: 0.03263282775878906 seconds\n",
      "\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "177 outlying rows have been removed\n",
      "* For test dataset\n",
      "264 outlying rows have been removed\n",
      "Outlier detection and removal done -- CPU time: 0.02551102638244629 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7017410015649452\n",
      "\n",
      "Classification done -- CPU time: 20.702768802642822 seconds\n",
      "End Pipeline CPU time: 20.771530151367188 seconds\n",
      "\n",
      "\n",
      "Strategy# 5 : Greedy traversal for starting state MM\n",
      "MM -> IQR -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      ">>Normalization \n",
      "* For train dataset\n",
      "... train dataset\n",
      "* For test dataset\n",
      "... test dataset\n",
      "Normalization done -- CPU time: 0.02725362777709961 seconds\n",
      "\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "177 outlying rows have been removed\n",
      "* For test dataset\n",
      "264 outlying rows have been removed\n",
      "Outlier detection and removal done -- CPU time: 0.02473902702331543 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7017410015649452\n",
      "\n",
      "Classification done -- CPU time: 20.916295528411865 seconds\n",
      "End Pipeline CPU time: 20.97903537750244 seconds\n",
      "\n",
      "\n",
      "Strategy# 6 : Greedy traversal for starting state ZS\n",
      "ZS -> LOF -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      ">>Normalization \n",
      "* For train dataset\n",
      "... train dataset\n",
      "* For test dataset\n",
      "... test dataset\n",
      "Normalization done -- CPU time: 0.02733635902404785 seconds\n",
      "\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "LOF requires no missing values, so missing values have been removed using DROP.\n",
      "30 outlying rows have been removed\n",
      "* For test dataset\n",
      "LOF requires no missing values, so missing values have been removed using DROP.\n",
      "30 outlying rows have been removed\n",
      "Outlier detection and removal done -- CPU time: 0.0219576358795166 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.6879166666666666\n",
      "\n",
      "Classification done -- CPU time: 17.405879020690918 seconds\n",
      "End Pipeline CPU time: 17.46433711051941 seconds\n",
      "\n",
      "\n",
      "Strategy# 7 : Greedy traversal for starting state MR\n",
      "MR -> DS -> IQR -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Feature selection \n",
      "Before feature selection:\n",
      "11 features \n",
      "Apply MR feature selection with missing threshold= 0.3\n",
      "1 features with greater than 0.30 missing values.\n",
      "\n",
      "List of variables to be removed : ['Cabin']\n",
      "After feature selection:\n",
      "10 features remain\n",
      "['Fare', 'Name', 'SibSp', 'Parch', 'PassengerId', 'Pclass', 'Ticket', 'Age', 'Embarked', 'Sex']\n",
      "Feature selection done -- CPU time: 0.006529569625854492 seconds\n",
      "\n",
      ">>Normalization \n",
      "* For train dataset\n",
      "... train dataset\n",
      "* For test dataset\n",
      "... test dataset\n",
      "Normalization done -- CPU time: 0.03188657760620117 seconds\n",
      "\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "177 outlying rows have been removed\n",
      "* For test dataset\n",
      "264 outlying rows have been removed\n",
      "Outlier detection and removal done -- CPU time: 0.024632692337036133 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7017410015649452\n",
      "\n",
      "Classification done -- CPU time: 20.76909613609314 seconds\n",
      "End Pipeline CPU time: 20.846691131591797 seconds\n",
      "\n",
      "\n",
      "Strategy# 8 : Greedy traversal for starting state WR\n",
      "WR -> MF -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Feature selection \n",
      "Before feature selection:\n",
      "11 features \n",
      "Warning: This strategy requires no missing values, so missing values have been removed applying DROP on the dataset.\n",
      "Apply WR feature selection\n",
      "Input variables must be non-negative. WR feature selection is only applied to positive variables.\n",
      "After feature selection:\n",
      "6 features remain\n",
      "['Fare', 'SibSp', 'Parch', 'PassengerId', 'Pclass', 'Age']\n",
      "Feature selection done -- CPU time: 0.015865802764892578 seconds\n",
      "\n",
      ">>Imputation \n",
      "* For train dataset\n",
      "Before imputation:\n",
      "No missing values in the given data\n",
      "* For test dataset\n",
      "Before imputation:\n",
      "Total 264 missing values in ['Fare', 'Age']\n",
      "- 264 numerical missing values in ['Fare', 'Age']\n",
      "After imputation:\n",
      "Total 0 missing values\n",
      "- 0 numerical missing values\n",
      "- 0.0 non-numerical missing values\n",
      "Imputation done -- CPU time: 0.02897357940673828 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7017410015649452\n",
      "\n",
      "Classification done -- CPU time: 22.303433179855347 seconds\n",
      "End Pipeline CPU time: 22.35914373397827 seconds\n",
      "\n",
      "\n",
      "Strategy# 9 : Greedy traversal for starting state LC\n",
      "LC -> MF -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Feature selection \n",
      "Before feature selection:\n",
      "11 features \n",
      "Apply LC feature selection with threshold= 0.3\n",
      "2 features with linear correlation greater than 0.30.\n",
      "\n",
      "List of correlated variables to be removed : ['Pclass', 'SibSp']\n",
      "After feature selection:\n",
      "9 features remain\n",
      "['Cabin', 'Fare', 'Name', 'Parch', 'PassengerId', 'Ticket', 'Age', 'Embarked', 'Sex']\n",
      "Feature selection done -- CPU time: 0.01772284507751465 seconds\n",
      "\n",
      ">>Imputation \n",
      "* For train dataset\n",
      "Before imputation:\n",
      "Total 866 missing values in ['Age', 'Cabin', 'Embarked']\n",
      "- 177 numerical missing values in ['Age']\n",
      "- 689 non-numerical missing values in ['Cabin', 'Embarked']\n",
      "After imputation:\n",
      "Total 0 missing values\n",
      "- 0 numerical missing values\n",
      "- 0 non-numerical missing values\n",
      "* For test dataset\n",
      "Before imputation:\n",
      "Total 1280 missing values in ['Cabin', 'Fare', 'Age', 'Embarked']\n",
      "- 264 numerical missing values in ['Fare', 'Age']\n",
      "- 1016 non-numerical missing values in ['Cabin', 'Embarked']\n",
      "After imputation:\n",
      "Total 0 missing values\n",
      "- 0 numerical missing values\n",
      "- 0 non-numerical missing values\n",
      "Imputation done -- CPU time: 0.07448506355285645 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.6902122347066166\n",
      "\n",
      "Classification done -- CPU time: 22.57688546180725 seconds\n",
      "End Pipeline CPU time: 22.680692672729492 seconds\n",
      "\n",
      "\n",
      "Strategy# 10 : Greedy traversal for starting state Tree\n",
      "Tree -> IQR -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "212 outlying rows have been removed\n",
      "* For test dataset\n",
      "297 outlying rows have been removed\n",
      "Outlier detection and removal done -- CPU time: 0.03162527084350586 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.6922714078374456\n",
      "\n",
      "Classification done -- CPU time: 21.00947856903076 seconds\n",
      "End Pipeline CPU time: 21.05218005180359 seconds\n",
      "\n",
      "\n",
      "Strategy# 11 : Greedy traversal for starting state ZSB\n",
      "ZSB -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "338 outlying rows have been removed:\n",
      "* For test dataset\n",
      "398 outlying rows have been removed:\n",
      "Outlier detection and removal done -- CPU time: 0.0312955379486084 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg accuracy of CART classification for 10 cross-validation : 0.7123693379790941\n",
      "\n",
      "Classification done -- CPU time: 20.366370916366577 seconds\n",
      "End Pipeline CPU time: 20.405399084091187 seconds\n",
      "\n",
      "\n",
      "Strategy# 12 : Greedy traversal for starting state LOF\n",
      "LOF -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "LOF requires no missing values, so missing values have been removed using DROP.\n",
      "30 outlying rows have been removed\n",
      "* For test dataset\n",
      "LOF requires no missing values, so missing values have been removed using DROP.\n",
      "30 outlying rows have been removed\n",
      "Outlier detection and removal done -- CPU time: 0.02336406707763672 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7204166666666667\n",
      "\n",
      "Classification done -- CPU time: 18.927054166793823 seconds\n",
      "End Pipeline CPU time: 18.957969188690186 seconds\n",
      "\n",
      "\n",
      "Strategy# 13 : Greedy traversal for starting state IQR\n",
      "IQR -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "212 outlying rows have been removed\n",
      "* For test dataset\n",
      "297 outlying rows have been removed\n",
      "Outlier detection and removal done -- CPU time: 0.030223608016967773 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.6922714078374456\n",
      "\n",
      "Classification done -- CPU time: 20.98859453201294 seconds\n",
      "End Pipeline CPU time: 21.026416301727295 seconds\n",
      "\n",
      "\n",
      "Strategy# 14 : Greedy traversal for starting state CC\n",
      "CC -> LOF -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      ">>Consistency checking\n",
      "Consistency checking done -- CPU time: 3.457069396972656e-05 seconds\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "LOF requires no missing values, so missing values have been removed using DROP.\n",
      "30 outlying rows have been removed\n",
      "* For test dataset\n",
      "LOF requires no missing values, so missing values have been removed using DROP.\n",
      "30 outlying rows have been removed\n",
      "Outlier detection and removal done -- CPU time: 0.02347540855407715 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7204166666666667\n",
      "\n",
      "Classification done -- CPU time: 18.923186540603638 seconds\n",
      "End Pipeline CPU time: 18.9569149017334 seconds\n",
      "\n",
      "\n",
      "Strategy# 15 : Greedy traversal for starting state PC\n",
      "PC -> ZSB -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      ">>Consistency checking\n",
      "Consistency checking done -- CPU time: 3.647804260253906e-05 seconds\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "338 outlying rows have been removed:\n",
      "* For test dataset\n",
      "398 outlying rows have been removed:\n",
      "Outlier detection and removal done -- CPU time: 0.031156063079833984 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7123693379790941\n",
      "\n",
      "Classification done -- CPU time: 20.312832593917847 seconds\n",
      "End Pipeline CPU time: 20.35492205619812 seconds\n",
      "\n",
      "\n",
      "Strategy# 16 : Greedy traversal for starting state ED\n",
      "ED -> KNN -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Duplicate detection and removal:\n",
      "* For train dataset\n",
      "Metric is not considered for 'ED'.\n",
      "Initial number of rows: 891\n",
      "After deduplication: Number of rows: 891\n",
      "* For test dataset\n",
      "Metric is not considered for 'ED'.\n",
      "Initial number of rows: 1309\n",
      "After deduplication: Number of rows: 1309\n",
      "Deduplication done -- CPU time: 0.09597325325012207 seconds\n",
      "\n",
      ">>Imputation \n",
      "* For train dataset\n",
      "Before imputation:\n",
      "Total 866 missing values in ['Age', 'Cabin', 'Embarked']\n",
      "- 177 numerical missing values in ['Age']\n",
      "- 689 non-numerical missing values in ['Cabin', 'Embarked']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fernandozagatti/anaconda3/lib/python3.7/site-packages/fancyimpute/solver.py:58: UserWarning: Input matrix is not missing any values\n",
      "  warnings.warn(\"Input matrix is not missing any values\")\n",
      "/home/fernandozagatti/anaconda3/lib/python3.7/site-packages/fancyimpute/solver.py:58: UserWarning: Input matrix is not missing any values\n",
      "  warnings.warn(\"Input matrix is not missing any values\")\n",
      "/home/fernandozagatti/anaconda3/lib/python3.7/site-packages/fancyimpute/solver.py:58: UserWarning: Input matrix is not missing any values\n",
      "  warnings.warn(\"Input matrix is not missing any values\")\n",
      "/home/fernandozagatti/anaconda3/lib/python3.7/site-packages/fancyimpute/solver.py:58: UserWarning: Input matrix is not missing any values\n",
      "  warnings.warn(\"Input matrix is not missing any values\")\n",
      "/home/fernandozagatti/anaconda3/lib/python3.7/site-packages/fancyimpute/solver.py:58: UserWarning: Input matrix is not missing any values\n",
      "  warnings.warn(\"Input matrix is not missing any values\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After imputation:\n",
      "Total 689 missing values\n",
      "- 0 numerical missing values\n",
      "- 689 non-numerical missing values\n",
      "* For test dataset\n",
      "Before imputation:\n",
      "Total 1280 missing values in ['Age', 'Cabin', 'Embarked', 'Fare']\n",
      "- 264 numerical missing values in ['Age', 'Fare']\n",
      "- 1016 non-numerical missing values in ['Cabin', 'Embarked']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fernandozagatti/anaconda3/lib/python3.7/site-packages/fancyimpute/solver.py:58: UserWarning: Input matrix is not missing any values\n",
      "  warnings.warn(\"Input matrix is not missing any values\")\n",
      "/home/fernandozagatti/anaconda3/lib/python3.7/site-packages/fancyimpute/solver.py:58: UserWarning: Input matrix is not missing any values\n",
      "  warnings.warn(\"Input matrix is not missing any values\")\n",
      "/home/fernandozagatti/anaconda3/lib/python3.7/site-packages/fancyimpute/solver.py:58: UserWarning: Input matrix is not missing any values\n",
      "  warnings.warn(\"Input matrix is not missing any values\")\n",
      "/home/fernandozagatti/anaconda3/lib/python3.7/site-packages/fancyimpute/solver.py:58: UserWarning: Input matrix is not missing any values\n",
      "  warnings.warn(\"Input matrix is not missing any values\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After imputation:\n",
      "Total 1016 missing values\n",
      "- 0 numerical missing values\n",
      "- 1016 non-numerical missing values\n",
      "Imputation done -- CPU time: 1.9205923080444336 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.6363795255930087\n",
      "\n",
      "Classification done -- CPU time: 22.312849044799805 seconds\n",
      "End Pipeline CPU time: 24.340892553329468 seconds\n",
      "\n",
      "\n",
      "Strategy# 17 : Greedy traversal for starting state AD\n",
      "AD -> ZS -> LOF -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Duplicate detection and removal:\n",
      "* For train dataset\n",
      "Metric is not considered for 'AD'.\n",
      "Number of duplicate rows removed: 0\n",
      "* For test dataset\n",
      "Metric is not considered for 'AD'.\n",
      "Number of duplicate rows removed: 0\n",
      "Deduplication done -- CPU time: 1.1692225933074951 seconds\n",
      "\n",
      ">>Normalization \n",
      "* For train dataset\n",
      "... train dataset\n",
      "* For test dataset\n",
      "... test dataset\n",
      "Normalization done -- CPU time: 0.03107929229736328 seconds\n",
      "\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "LOF requires no missing values, so missing values have been removed using DROP.\n",
      "30 outlying rows have been removed\n",
      "* For test dataset\n",
      "LOF requires no missing values, so missing values have been removed using DROP.\n",
      "30 outlying rows have been removed\n",
      "Outlier detection and removal done -- CPU time: 0.023166179656982422 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.6879166666666666\n",
      "\n",
      "Classification done -- CPU time: 17.53754210472107 seconds\n",
      "End Pipeline CPU time: 18.77581262588501 seconds\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7017410015649452\n",
      "\n",
      "Classification done -- CPU time: 23.5494065284729 seconds\n",
      "End Pipeline CPU time: 23.55330801010132 seconds\n",
      "\n",
      "==== Recap ====\n",
      "\n",
      "List of strategies tried by Learn2Clean:\n",
      "['MICE -> CART', 'EM -> MM -> IQR -> CART', 'KNN -> CART', 'MF -> CART', 'DS -> IQR -> CART', 'MM -> IQR -> CART', 'ZS -> LOF -> CART', 'MR -> DS -> IQR -> CART', 'WR -> MF -> CART', 'LC -> MF -> CART', 'Tree -> IQR -> CART', 'ZSB -> CART', 'LOF -> CART', 'IQR -> CART', 'CC -> LOF -> CART', 'PC -> ZSB -> CART', 'ED -> KNN -> CART', 'AD -> ZS -> LOF -> CART']\n",
      "\n",
      "List of corresponding quality metrics ****\n",
      " [{'quality_metric': 0.7047940074906367}, {'quality_metric': 0.7000000000000001}, {'quality_metric': 0.6363795255930087}, {'quality_metric': 0.7047940074906367}, {'quality_metric': 0.7017410015649452}, {'quality_metric': 0.7017410015649452}, {'quality_metric': 0.6879166666666666}, {'quality_metric': 0.7017410015649452}, {'quality_metric': 0.7017410015649452}, {'quality_metric': 0.6902122347066166}, {'quality_metric': 0.6922714078374456}, {'quality_metric': 0.7123693379790941}, {'quality_metric': 0.7204166666666667}, {'quality_metric': 0.6922714078374456}, {'quality_metric': 0.7204166666666667}, {'quality_metric': 0.7123693379790941}, {'quality_metric': 0.6363795255930087}, {'quality_metric': 0.6879166666666666}, {'quality_metric': 0.7017410015649452}]\n",
      "\n",
      "Strategy LOF -> CART for maximal accuracy : 0.7204166666666667 for CART\n",
      "\n",
      "=== End of Learn2Clean - Pipeline execution -- CPU time: 405.5795907974243 seconds\n",
      "\n",
      "**** Best strategy ****\n",
      "('titanic_example', 'learn2clean', 'CART', 'Survived', None, 'LOF -> CART', 'accuracy', 0.7204166666666667, 405.5795907974243)\n"
     ]
    }
   ],
   "source": [
    "import learn2clean.loading.reader as rd \n",
    "import learn2clean.qlearning.qlearner as ql\n",
    "\n",
    "titanic = [\"csv_results/titanic_train.csv\",\"csv_results/titanic_test.csv\"]\n",
    "hr=rd.Reader(sep=',',verbose=False, encoding=False) \n",
    "dataset=hr.train_test_split(titanic, 'Survived')\n",
    "\n",
    "l2c_c1assification1=ql.Qlearner(dataset = dataset,goal='CART', target_goal='Survived',threshold = 0.6, target_prepare=None, file_name = 'titanic_example', verbose = False)\n",
    "l2c_c1assification1.learn2clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Tic-Tac-Toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Learn2Clean\n",
      "Learn2Clean - Pipeline construction -- CPU time: 0.22894859313964844 seconds\n",
      "=== Start Pipeline Execution ===\n",
      "\n",
      "\n",
      "Strategy# 0 : Greedy traversal for starting state DS\n",
      "DS -> IQR -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      ">>Normalization \n",
      "* For train dataset\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "at least one array or dtype is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-27bdd8fa9297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0ml2c_c1assification1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQlearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgoal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CART'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_goal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Class'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_prepare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tictactoe_example'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0ml2c_c1assification1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn2clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/learn2clean/qlearning/qlearner.py\u001b[0m in \u001b[0;36mlearn2clean\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         result_list = self.show_traverse(self.dataset, q, g, self.target_goal,\n\u001b[0;32m--> 635\u001b[0;31m                                          self.target_prepare, check_missing)\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0mquality_metric_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/learn2clean/qlearning/qlearner.py\u001b[0m in \u001b[0;36mshow_traverse\u001b[0;34m(self, dataset, q, g, target1, target2, check_missing)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             strategy.append(self.pipeline(dataset, actions_list, target1,\n\u001b[0;32m--> 496\u001b[0;31m                                           target2, check_missing)[1])\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         strategy.append(self.pipeline(dataset, [g+len(methods)-1], target1,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/learn2clean/qlearning/qlearner.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(self, dataset, actions_list, target_goal, target_prepare, check_missing)\u001b[0m\n\u001b[1;32m    313\u001b[0m                     n = L2C_class[a](dataset=dataset, strategy=actions_name[a],\n\u001b[1;32m    314\u001b[0m                                      \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_prepare\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                                      verbose=self.verbose).transform()\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/learn2clean/normalization/normalizer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"DS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                     \u001b[0mdn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDS_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ZS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/learn2clean/normalization/normalizer.py\u001b[0m in \u001b[0;36mDS_normalization\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             scaled_values = quantile_transform(\n\u001b[0;32m--> 207\u001b[0;31m                 X, n_quantiles=10, random_state=0)\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             scaled_X = pd.DataFrame(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mquantile_transform\u001b[0;34m(X, axis, n_quantiles, output_distribution, ignore_implicit_zeros, subsample, random_state, copy)\u001b[0m\n\u001b[1;32m   2688\u001b[0m                             copy=copy)\n\u001b[1;32m   2689\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2690\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2691\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   2361\u001b[0m                                                        self.subsample))\n\u001b[1;32m   2362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_fit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36m_check_inputs\u001b[0;34m(self, X, in_fit, accept_sparse_negative, copy)\u001b[0m\n\u001b[1;32m   2467\u001b[0m                                 \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2468\u001b[0m                                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2469\u001b[0;31m                                 force_all_finite='allow-nan')\n\u001b[0m\u001b[1;32m   2470\u001b[0m         \u001b[0;31m# we only accept positive sparse matrix when ignore_implicit_zeros is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m         \u001b[0;31m# false and that we call fit or transform.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    418\u001b[0m                     \u001b[0;34mf\"requires y to be passed, but the target y is None.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                 )\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0mdtype_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mresult_type\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: at least one array or dtype is required"
     ]
    }
   ],
   "source": [
    "import learn2clean.loading.reader as rd \n",
    "import learn2clean.qlearning.qlearner as ql\n",
    "\n",
    "tictactoe = [\"csv_results/tictactoe_train.csv\",\"csv_results/tictactoe_test.csv\"]\n",
    "hr=rd.Reader(sep=',',verbose=False, encoding=False) \n",
    "dataset=hr.train_test_split(tictactoe, 'Class')\n",
    "\n",
    "l2c_c1assification1=ql.Qlearner(dataset = dataset,goal='CART', target_goal='Class',threshold = 0.6, target_prepare=None, file_name = 'tictactoe_example', verbose = False)\n",
    "l2c_c1assification1.learn2clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Bike Buyers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Learn2Clean\n",
      "Learn2Clean - Pipeline construction -- CPU time: 0.21088290214538574 seconds\n",
      "=== Start Pipeline Execution ===\n",
      "\n",
      "\n",
      "Strategy# 0 : Greedy traversal for starting state DS\n",
      "DS -> IQR -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      ">>Normalization \n",
      "* For train dataset\n",
      "... train dataset\n",
      "* For test dataset\n",
      "... test dataset\n",
      "Normalization done -- CPU time: 0.04099631309509277 seconds\n",
      "\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "0 outlying rows have been removed\n",
      "* For test dataset\n",
      "0 outlying rows have been removed\n",
      "Outlier detection and removal done -- CPU time: 0.02254343032836914 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.651\n",
      "\n",
      "Classification done -- CPU time: 25.744238138198853 seconds\n",
      "End Pipeline CPU time: 25.807942152023315 seconds\n",
      "\n",
      "\n",
      "Strategy# 1 : Greedy traversal for starting state MM\n",
      "MM -> AD -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      ">>Normalization \n",
      "* For train dataset\n",
      "... train dataset\n",
      "* For test dataset\n",
      "... test dataset\n",
      "Normalization done -- CPU time: 0.02734065055847168 seconds\n",
      "\n",
      "\n",
      ">>Duplicate detection and removal:\n",
      "* For train dataset\n",
      "Metric is not considered for 'AD'.\n",
      "Number of duplicate rows removed: 0\n",
      "* For test dataset\n",
      "Metric is not considered for 'AD'.\n",
      "Number of duplicate rows removed: 0\n",
      "Deduplication done -- CPU time: 1.1648976802825928 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.65\n",
      "\n",
      "Classification done -- CPU time: 24.272796154022217 seconds\n",
      "End Pipeline CPU time: 25.4652099609375 seconds\n",
      "\n",
      "\n",
      "Strategy# 2 : Greedy traversal for starting state ZS\n",
      "ZS -> ED -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      ">>Normalization \n",
      "* For train dataset\n",
      "... train dataset\n",
      "* For test dataset\n",
      "... test dataset\n",
      "Normalization done -- CPU time: 0.028628826141357422 seconds\n",
      "\n",
      "\n",
      ">>Duplicate detection and removal:\n",
      "* For train dataset\n",
      "Metric is not considered for 'ED'.\n",
      "Initial number of rows: 1000\n",
      "After deduplication: Number of rows: 1000\n",
      "* For test dataset\n",
      "Metric is not considered for 'ED'.\n",
      "Initial number of rows: 1000\n",
      "After deduplication: Number of rows: 1000\n",
      "Deduplication done -- CPU time: 0.007630109786987305 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.6489999999999999\n",
      "\n",
      "Classification done -- CPU time: 24.342844009399414 seconds\n",
      "End Pipeline CPU time: 24.37924313545227 seconds\n",
      "\n",
      "\n",
      "Strategy# 3 : Greedy traversal for starting state WR\n",
      "WR -> IQR -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Feature selection \n",
      "Before feature selection:\n",
      "12 features \n",
      "Apply WR feature selection\n",
      "Input variables must be non-negative. WR feature selection is only applied to positive variables.\n",
      "After feature selection:\n",
      "5 features remain\n",
      "['Children', 'Age', 'ID', 'Cars', 'Income']\n",
      "Feature selection done -- CPU time: 0.01589179039001465 seconds\n",
      "\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "69 outlying rows have been removed\n",
      "* For test dataset\n",
      "69 outlying rows have been removed\n",
      "Outlier detection and removal done -- CPU time: 0.02012324333190918 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.6627659574468086\n",
      "\n",
      "Classification done -- CPU time: 24.05672836303711 seconds\n",
      "End Pipeline CPU time: 24.094696760177612 seconds\n",
      "\n",
      "\n",
      "Strategy# 4 : Greedy traversal for starting state LC\n",
      "LC -> LOF -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Feature selection \n",
      "Before feature selection:\n",
      "12 features \n",
      "Apply LC feature selection with threshold= 0.3\n",
      "2 features with linear correlation greater than 0.30.\n",
      "\n",
      "List of correlated variables to be removed : ['Children', 'Income']\n",
      "After feature selection:\n",
      "10 features remain\n",
      "['Marital Status', 'Education', 'Age', 'Gender', 'Commute Distance', 'ID', 'Cars', 'Occupation', 'Region', 'Home Owner']\n",
      "Feature selection done -- CPU time: 0.017675161361694336 seconds\n",
      "\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "30 outlying rows have been removed\n",
      "* For test dataset\n",
      "30 outlying rows have been removed\n",
      "Outlier detection and removal done -- CPU time: 0.021940946578979492 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.6298969072164948\n",
      "\n",
      "Classification done -- CPU time: 24.324451446533203 seconds\n",
      "End Pipeline CPU time: 24.364527702331543 seconds\n",
      "\n",
      "\n",
      "Strategy# 5 : Greedy traversal for starting state Tree\n",
      "Tree -> ED -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Duplicate detection and removal:\n",
      "* For train dataset\n",
      "Metric is not considered for 'ED'.\n",
      "Initial number of rows: 1000\n",
      "After deduplication: Number of rows: 1000\n",
      "* For test dataset\n",
      "Metric is not considered for 'ED'.\n",
      "Initial number of rows: 1000\n",
      "After deduplication: Number of rows: 1000\n",
      "Deduplication done -- CPU time: 0.00850224494934082 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.65\n",
      "\n",
      "Classification done -- CPU time: 27.17180323600769 seconds\n",
      "End Pipeline CPU time: 27.18038249015808 seconds\n",
      "\n",
      "\n",
      "Strategy# 6 : Greedy traversal for starting state ZSB\n",
      "ZSB -> ED -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "64 outlying rows have been removed:\n",
      "* For test dataset\n",
      "64 outlying rows have been removed:\n",
      "Outlier detection and removal done -- CPU time: 0.02772831916809082 seconds\n",
      "\n",
      "\n",
      ">>Duplicate detection and removal:\n",
      "* For train dataset\n",
      "Metric is not considered for 'ED'.\n",
      "Initial number of rows: 936\n",
      "After deduplication: Number of rows: 936\n",
      "* For test dataset\n",
      "Metric is not considered for 'ED'.\n",
      "Initial number of rows: 936\n",
      "After deduplication: Number of rows: 936\n",
      "Deduplication done -- CPU time: 0.0074770450592041016 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.6816975520475864\n",
      "\n",
      "Classification done -- CPU time: 24.660781145095825 seconds\n",
      "End Pipeline CPU time: 24.69614291191101 seconds\n",
      "\n",
      "\n",
      "Strategy# 7 : Greedy traversal for starting state LOF\n",
      "LOF -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "30 outlying rows have been removed\n",
      "* For test dataset\n",
      "30 outlying rows have been removed\n",
      "Outlier detection and removal done -- CPU time: 0.02516484260559082 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.6515463917525773\n",
      "\n",
      "Classification done -- CPU time: 25.057164192199707 seconds\n",
      "End Pipeline CPU time: 25.0824134349823 seconds\n",
      "\n",
      "\n",
      "Strategy# 8 : Greedy traversal for starting state IQR\n",
      "IQR -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "69 outlying rows have been removed\n",
      "* For test dataset\n",
      "69 outlying rows have been removed\n",
      "Outlier detection and removal done -- CPU time: 0.024168968200683594 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.6627659574468086\n",
      "\n",
      "Classification done -- CPU time: 25.13975954055786 seconds\n",
      "End Pipeline CPU time: 25.164026021957397 seconds\n",
      "\n",
      "\n",
      "Strategy# 9 : Greedy traversal for starting state CC\n",
      "CC -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      ">>Consistency checking\n",
      "Consistency checking done -- CPU time: 2.5510787963867188e-05 seconds\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.65\n",
      "\n",
      "Classification done -- CPU time: 28.11953639984131 seconds\n",
      "End Pipeline CPU time: 28.119650840759277 seconds\n",
      "\n",
      "\n",
      "Strategy# 10 : Greedy traversal for starting state PC\n",
      "PC -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      ">>Consistency checking\n",
      "Consistency checking done -- CPU time: 1.6450881958007812e-05 seconds\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.65\n",
      "\n",
      "Classification done -- CPU time: 27.20669436454773 seconds\n",
      "End Pipeline CPU time: 27.20677876472473 seconds\n",
      "\n",
      "\n",
      "Strategy# 11 : Greedy traversal for starting state ED\n",
      "ED -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Duplicate detection and removal:\n",
      "* For train dataset\n",
      "Metric is not considered for 'ED'.\n",
      "Initial number of rows: 1000\n",
      "After deduplication: Number of rows: 1000\n",
      "* For test dataset\n",
      "Metric is not considered for 'ED'.\n",
      "Initial number of rows: 1000\n",
      "After deduplication: Number of rows: 1000\n",
      "Deduplication done -- CPU time: 0.0076143741607666016 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.65\n",
      "\n",
      "Classification done -- CPU time: 26.1434383392334 seconds\n",
      "End Pipeline CPU time: 26.151142835617065 seconds\n",
      "\n",
      "\n",
      "Strategy# 12 : Greedy traversal for starting state AD\n",
      "AD -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Duplicate detection and removal:\n",
      "* For train dataset\n",
      "Metric is not considered for 'AD'.\n",
      "Number of duplicate rows removed: 0\n",
      "* For test dataset\n",
      "Metric is not considered for 'AD'.\n",
      "Number of duplicate rows removed: 0\n",
      "Deduplication done -- CPU time: 0.19701457023620605 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg accuracy of CART classification for 10 cross-validation : 0.65\n",
      "\n",
      "Classification done -- CPU time: 25.03955340385437 seconds\n",
      "End Pipeline CPU time: 25.23687243461609 seconds\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.643\n",
      "\n",
      "Classification done -- CPU time: 26.90048599243164 seconds\n",
      "End Pipeline CPU time: 26.900529146194458 seconds\n",
      "\n",
      "==== Recap ====\n",
      "\n",
      "List of strategies tried by Learn2Clean:\n",
      "['DS -> IQR -> CART', 'MM -> AD -> CART', 'ZS -> ED -> CART', 'WR -> IQR -> CART', 'LC -> LOF -> CART', 'Tree -> ED -> CART', 'ZSB -> ED -> CART', 'LOF -> CART', 'IQR -> CART', 'CC -> CART', 'PC -> CART', 'ED -> CART', 'AD -> CART']\n",
      "\n",
      "List of corresponding quality metrics ****\n",
      " [{'quality_metric': 0.651}, {'quality_metric': 0.65}, {'quality_metric': 0.6489999999999999}, {'quality_metric': 0.6627659574468086}, {'quality_metric': 0.6298969072164948}, {'quality_metric': 0.65}, {'quality_metric': 0.6816975520475864}, {'quality_metric': 0.6515463917525773}, {'quality_metric': 0.6627659574468086}, {'quality_metric': 0.65}, {'quality_metric': 0.65}, {'quality_metric': 0.65}, {'quality_metric': 0.65}, {'quality_metric': 0.643}]\n",
      "\n",
      "Strategy ZSB -> ED -> CART for maximal accuracy : 0.6816975520475864 for CART\n",
      "\n",
      "=== End of Learn2Clean - Pipeline execution -- CPU time: 359.85470390319824 seconds\n",
      "\n",
      "**** Best strategy ****\n",
      "('bike_example', 'learn2clean', 'CART', 'Purchased Bike', None, 'ZSB -> ED -> CART', 'accuracy', 0.6816975520475864, 359.85470390319824)\n"
     ]
    }
   ],
   "source": [
    "import learn2clean.loading.reader as rd \n",
    "import learn2clean.qlearning.qlearner as ql\n",
    "\n",
    "bike = [\"csv_results/bikebuyers_train.csv\",\"csv_results/bikebuyers_test.csv\"]\n",
    "hr=rd.Reader(sep=',',verbose=False, encoding=False) \n",
    "dataset=hr.train_test_split(bike, 'Purchased Bike')\n",
    "\n",
    "l2c_c1assification1=ql.Qlearner(dataset = dataset,goal='CART', target_goal='Purchased Bike',threshold = 0.6, target_prepare=None, file_name = 'bike_example', verbose = False)\n",
    "l2c_c1assification1.learn2clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Cardiovascular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Learn2Clean\n",
      "Learn2Clean - Pipeline construction -- CPU time: 0.26255083084106445 seconds\n",
      "=== Start Pipeline Execution ===\n",
      "\n",
      "\n",
      "Strategy# 0 : Greedy traversal for starting state DS\n",
      "DS -> IQR -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      ">>Normalization \n",
      "* For train dataset\n",
      "... train dataset\n",
      "* For test dataset\n",
      "... test dataset\n",
      "Normalization done -- CPU time: 0.30000996589660645 seconds\n",
      "\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "0 outlying rows have been removed\n",
      "* For test dataset\n",
      "0 outlying rows have been removed\n",
      "Outlier detection and removal done -- CPU time: 0.589040994644165 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7318285714285715\n",
      "\n",
      "Classification done -- CPU time: 636.9895057678223 seconds\n",
      "End Pipeline CPU time: 637.8789820671082 seconds\n",
      "\n",
      "\n",
      "Strategy# 1 : Greedy traversal for starting state MM\n",
      "MM -> AD -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      ">>Normalization \n",
      "* For train dataset\n",
      "... train dataset\n",
      "* For test dataset\n",
      "... test dataset\n",
      "Normalization done -- CPU time: 0.08780264854431152 seconds\n",
      "\n",
      "\n",
      ">>Duplicate detection and removal:\n",
      "* For train dataset\n",
      "Metric is not considered for 'AD'.\n",
      "Number of duplicate rows removed: 0\n",
      "* For test dataset\n",
      "Metric is not considered for 'AD'.\n",
      "Number of duplicate rows removed: 0\n",
      "Deduplication done -- CPU time: 8.77866244316101 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7318428571428571\n",
      "\n",
      "Classification done -- CPU time: 625.521089553833 seconds\n",
      "End Pipeline CPU time: 634.3877897262573 seconds\n",
      "\n",
      "\n",
      "Strategy# 2 : Greedy traversal for starting state ZS\n",
      "ZS -> ED -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      ">>Normalization \n",
      "* For train dataset\n",
      "... train dataset\n",
      "* For test dataset\n",
      "... test dataset\n",
      "Normalization done -- CPU time: 0.1051638126373291 seconds\n",
      "\n",
      "\n",
      ">>Duplicate detection and removal:\n",
      "* For train dataset\n",
      "Metric is not considered for 'ED'.\n",
      "Initial number of rows: 70000\n",
      "After deduplication: Number of rows: 70000\n",
      "* For test dataset\n",
      "Metric is not considered for 'ED'.\n",
      "Initial number of rows: 70000\n",
      "After deduplication: Number of rows: 70000\n",
      "Deduplication done -- CPU time: 0.03453230857849121 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7318428571428571\n",
      "\n",
      "Classification done -- CPU time: 627.2810754776001 seconds\n",
      "End Pipeline CPU time: 627.420952796936 seconds\n",
      "\n",
      "\n",
      "Strategy# 3 : Greedy traversal for starting state WR\n",
      "WR -> IQR -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Feature selection \n",
      "Before feature selection:\n",
      "12 features \n",
      "Apply WR feature selection\n",
      "Input variables must be non-negative. WR feature selection is only applied to positive variables.\n",
      "Not possible identify best subset of features\n",
      "After feature selection:\n",
      "12 features remain\n",
      "['height', 'gender', 'ap_lo', 'cholesterol', 'ap_hi', 'smoke', 'weight', 'id', 'gluc', 'active', 'alco', 'age']\n",
      "Feature selection done -- CPU time: 0.09926223754882812 seconds\n",
      "\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "8069 outlying rows have been removed\n",
      "* For test dataset\n",
      "8069 outlying rows have been removed\n",
      "Outlier detection and removal done -- CPU time: 0.21576285362243652 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.725145665570422\n",
      "\n",
      "Classification done -- CPU time: 551.9506893157959 seconds\n",
      "End Pipeline CPU time: 552.2670004367828 seconds\n",
      "\n",
      "\n",
      "Strategy# 4 : Greedy traversal for starting state LC\n",
      "LC -> LOF -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Feature selection \n",
      "Before feature selection:\n",
      "12 features \n",
      "Apply LC feature selection with threshold= 0.3\n",
      "3 features with linear correlation greater than 0.30.\n",
      "\n",
      "List of correlated variables to be removed : ['gluc', 'height', 'smoke']\n",
      "After feature selection:\n",
      "9 features remain\n",
      "['gender', 'ap_lo', 'cholesterol', 'ap_hi', 'weight', 'id', 'alco', 'active', 'age']\n",
      "Feature selection done -- CPU time: 0.14531445503234863 seconds\n",
      "\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "30 outlying rows have been removed\n",
      "* For test dataset\n",
      "30 outlying rows have been removed\n",
      "Outlier detection and removal done -- CPU time: 3.0050501823425293 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7314277547520367\n",
      "\n",
      "Classification done -- CPU time: 553.4648835659027 seconds\n",
      "End Pipeline CPU time: 556.6170756816864 seconds\n",
      "\n",
      "\n",
      "Strategy# 5 : Greedy traversal for starting state Tree\n",
      "Tree -> ED -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Duplicate detection and removal:\n",
      "* For train dataset\n",
      "Metric is not considered for 'ED'.\n",
      "Initial number of rows: 70000\n",
      "After deduplication: Number of rows: 70000\n",
      "* For test dataset\n",
      "Metric is not considered for 'ED'.\n",
      "Initial number of rows: 70000\n",
      "After deduplication: Number of rows: 70000\n",
      "Deduplication done -- CPU time: 0.03441190719604492 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7318428571428571\n",
      "\n",
      "Classification done -- CPU time: 635.6745367050171 seconds\n",
      "End Pipeline CPU time: 635.7090508937836 seconds\n",
      "\n",
      "\n",
      "Strategy# 6 : Greedy traversal for starting state ZSB\n",
      "ZSB -> ED -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "12392 outlying rows have been removed:\n",
      "* For test dataset\n",
      "12392 outlying rows have been removed:\n",
      "Outlier detection and removal done -- CPU time: 0.25440549850463867 seconds\n",
      "\n",
      "\n",
      ">>Duplicate detection and removal:\n",
      "* For train dataset\n",
      "Metric is not considered for 'ED'.\n",
      "Initial number of rows: 57608\n",
      "After deduplication: Number of rows: 57608\n",
      "* For test dataset\n",
      "Metric is not considered for 'ED'.\n",
      "Initial number of rows: 57608\n",
      "After deduplication: Number of rows: 57608\n",
      "Deduplication done -- CPU time: 0.03507518768310547 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7214100621516326\n",
      "\n",
      "Classification done -- CPU time: 507.21304750442505 seconds\n",
      "End Pipeline CPU time: 507.502699136734 seconds\n",
      "\n",
      "\n",
      "Strategy# 7 : Greedy traversal for starting state LOF\n",
      "LOF -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "30 outlying rows have been removed\n",
      "* For test dataset\n",
      "30 outlying rows have been removed\n",
      "Outlier detection and removal done -- CPU time: 3.8975257873535156 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7324853508646563\n",
      "\n",
      "Classification done -- CPU time: 633.0204043388367 seconds\n",
      "End Pipeline CPU time: 636.9180326461792 seconds\n",
      "\n",
      "\n",
      "Strategy# 8 : Greedy traversal for starting state IQR\n",
      "IQR -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Outlier detection and removal:\n",
      "* For train dataset\n",
      "8069 outlying rows have been removed\n",
      "* For test dataset\n",
      "8069 outlying rows have been removed\n",
      "Outlier detection and removal done -- CPU time: 0.18749690055847168 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.725145665570422\n",
      "\n",
      "Classification done -- CPU time: 550.6504635810852 seconds\n",
      "End Pipeline CPU time: 550.8380937576294 seconds\n",
      "\n",
      "\n",
      "Strategy# 9 : Greedy traversal for starting state CC\n",
      "CC -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      ">>Consistency checking\n",
      "Consistency checking done -- CPU time: 1.8358230590820312e-05 seconds\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7318428571428571\n",
      "\n",
      "Classification done -- CPU time: 669.1716103553772 seconds\n",
      "End Pipeline CPU time: 669.1717019081116 seconds\n",
      "\n",
      "\n",
      "Strategy# 10 : Greedy traversal for starting state PC\n",
      "PC -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      ">>Consistency checking\n",
      "Consistency checking done -- CPU time: 1.6450881958007812e-05 seconds\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7318428571428571\n",
      "\n",
      "Classification done -- CPU time: 640.0928013324738 seconds\n",
      "End Pipeline CPU time: 640.0928955078125 seconds\n",
      "\n",
      "\n",
      "Strategy# 11 : Greedy traversal for starting state ED\n",
      "ED -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Duplicate detection and removal:\n",
      "* For train dataset\n",
      "Metric is not considered for 'ED'.\n",
      "Initial number of rows: 70000\n",
      "After deduplication: Number of rows: 70000\n",
      "* For test dataset\n",
      "Metric is not considered for 'ED'.\n",
      "Initial number of rows: 70000\n",
      "After deduplication: Number of rows: 70000\n",
      "Deduplication done -- CPU time: 0.03302764892578125 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg accuracy of CART classification for 10 cross-validation : 0.7318428571428571\n",
      "\n",
      "Classification done -- CPU time: 649.6619243621826 seconds\n",
      "End Pipeline CPU time: 649.6950421333313 seconds\n",
      "\n",
      "\n",
      "Strategy# 12 : Greedy traversal for starting state AD\n",
      "AD -> CART\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Duplicate detection and removal:\n",
      "* For train dataset\n",
      "Metric is not considered for 'AD'.\n",
      "Number of duplicate rows removed: 0\n",
      "* For test dataset\n",
      "Metric is not considered for 'AD'.\n",
      "Number of duplicate rows removed: 0\n",
      "Deduplication done -- CPU time: 7.8735411167144775 seconds\n",
      "\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7318428571428571\n",
      "\n",
      "Classification done -- CPU time: 661.0429430007935 seconds\n",
      "End Pipeline CPU time: 668.9169759750366 seconds\n",
      "\n",
      "Start pipeline\n",
      "-------------\n",
      "\n",
      ">>Classification task\n",
      "Avg accuracy of CART classification for 10 cross-validation : 0.7317428571428571\n",
      "\n",
      "Classification done -- CPU time: 840.1895697116852 seconds\n",
      "End Pipeline CPU time: 840.1896109580994 seconds\n",
      "\n",
      "==== Recap ====\n",
      "\n",
      "List of strategies tried by Learn2Clean:\n",
      "['DS -> IQR -> CART', 'MM -> AD -> CART', 'ZS -> ED -> CART', 'WR -> IQR -> CART', 'LC -> LOF -> CART', 'Tree -> ED -> CART', 'ZSB -> ED -> CART', 'LOF -> CART', 'IQR -> CART', 'CC -> CART', 'PC -> CART', 'ED -> CART', 'AD -> CART']\n",
      "\n",
      "List of corresponding quality metrics ****\n",
      " [{'quality_metric': 0.7318285714285715}, {'quality_metric': 0.7318428571428571}, {'quality_metric': 0.7318428571428571}, {'quality_metric': 0.725145665570422}, {'quality_metric': 0.7314277547520367}, {'quality_metric': 0.7318428571428571}, {'quality_metric': 0.7214100621516326}, {'quality_metric': 0.7324853508646563}, {'quality_metric': 0.725145665570422}, {'quality_metric': 0.7318428571428571}, {'quality_metric': 0.7318428571428571}, {'quality_metric': 0.7318428571428571}, {'quality_metric': 0.7318428571428571}, {'quality_metric': 0.7317428571428571}]\n",
      "\n",
      "Strategy LOF -> CART for maximal accuracy : 0.7324853508646563 for CART\n",
      "\n",
      "=== End of Learn2Clean - Pipeline execution -- CPU time: 8807.612810373306 seconds\n",
      "\n",
      "**** Best strategy ****\n",
      "('cardio_example', 'learn2clean', 'CART', 'cardio', None, 'LOF -> CART', 'accuracy', 0.7324853508646563, 8807.612810373306)\n"
     ]
    }
   ],
   "source": [
    "import learn2clean.loading.reader as rd \n",
    "import learn2clean.qlearning.qlearner as ql\n",
    "\n",
    "cardio = [\"csv_results/cardio_train.csv\",\"csv_results/cardio_test.csv\"]\n",
    "hr=rd.Reader(sep=',',verbose=False, encoding=False) \n",
    "dataset=hr.train_test_split(cardio, 'cardio')\n",
    "\n",
    "l2c_c1assification1=ql.Qlearner(dataset = dataset,goal='CART', target_goal='cardio',threshold = 0.6, target_prepare=None, file_name = 'cardio_example', verbose = False)\n",
    "l2c_c1assification1.learn2clean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
