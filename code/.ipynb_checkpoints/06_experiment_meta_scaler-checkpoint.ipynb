{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26399be1-b485-44e3-a488-935b6eae47a5",
   "metadata": {},
   "source": [
    "Multiclass, single label. All metafeatures (pymfe and ImbCoL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0855441-cd80-4306-8344-f284625279c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install deslib pandas matplotlib xgboost==1.0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8aef68-30c1-47fd-ac66-0766df66e431",
   "metadata": {},
   "source": [
    "# Importing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1112257c-cb89-41e6-9504-e8930af3fcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/livia/home/vision/Ldeamorim/miniconda3/envs/meta_scaler/lib/python3.8/site-packages/xgboost/compat.py:93: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#from sklearn.model_selection import cross_validate\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "#from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from deslib.des.knora_e import KNORAE\n",
    "from deslib.des.knora_u import KNORAU\n",
    "from deslib.des.meta_des import METADES\n",
    "#from deslib.des.des_mi import DESMI\n",
    "from des_mi import DESMI\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from sklearn.utils import class_weight\n",
    "#from sklearn.feature_selection import SelectKBest\n",
    "#from sklearn.feature_selection import chi2\n",
    "#from imblearn.over_sampling import SM?OTE\n",
    "\n",
    "#import shap\n",
    "#import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a5eb6a-0bda-41ea-83ec-3027ab6ff780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library versions:\n",
      "python 3.8.10\n",
      "sklearn  1.1.2\n",
      "deslib  0.3.5\n",
      "xgboost  1.0.2\n",
      "numpy  1.21.5\n",
      "pandas  1.5.3\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "import sklearn\n",
    "import deslib\n",
    "import xgboost\n",
    "import numpy\n",
    "import pandas\n",
    "print('Library versions:')\n",
    "print('python', python_version())\n",
    "print('sklearn ',sklearn. __version__ )\n",
    "print('deslib ',deslib.__version__)\n",
    "print('xgboost ',xgboost.__version__)\n",
    "print('numpy ',numpy.__version__)\n",
    "print('pandas ', pandas.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eed69f3-5f8a-4766-8f33-5aae56e5556b",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "304b348b-63ff-475f-994a-79290a09c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('metafeat_pymfe+imbcol_and_ST_perform_for_pairs_of_dataset_and_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50ad6269-ff32-48fa-b039-557a84710e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>attr_conc.mean</th>\n",
       "      <th>attr_ent.mean</th>\n",
       "      <th>attr_to_inst</th>\n",
       "      <th>best_node.mean</th>\n",
       "      <th>best_node.mean.relative</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>can_cor.mean</th>\n",
       "      <th>...</th>\n",
       "      <th>linearity.class.L3_partial.1</th>\n",
       "      <th>NS</th>\n",
       "      <th>SS</th>\n",
       "      <th>MMS</th>\n",
       "      <th>MAS</th>\n",
       "      <th>RS</th>\n",
       "      <th>QT</th>\n",
       "      <th>Max_F1_perf</th>\n",
       "      <th>Best_STs</th>\n",
       "      <th>Best_ST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bagging</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.019398</td>\n",
       "      <td>2.584913</td>\n",
       "      <td>0.066445</td>\n",
       "      <td>0.494839</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.999928</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448505</td>\n",
       "      <td>0.440079</td>\n",
       "      <td>0.466025</td>\n",
       "      <td>0.394689</td>\n",
       "      <td>0.303649</td>\n",
       "      <td>0.449671</td>\n",
       "      <td>0.451235</td>\n",
       "      <td>0.466025</td>\n",
       "      <td>['SS']</td>\n",
       "      <td>SS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GLVQ</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.019398</td>\n",
       "      <td>2.584913</td>\n",
       "      <td>0.066445</td>\n",
       "      <td>0.494839</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.999928</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448505</td>\n",
       "      <td>0.434862</td>\n",
       "      <td>0.462094</td>\n",
       "      <td>0.418003</td>\n",
       "      <td>0.445430</td>\n",
       "      <td>0.469747</td>\n",
       "      <td>0.487364</td>\n",
       "      <td>0.487364</td>\n",
       "      <td>['QT']</td>\n",
       "      <td>QT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.019398</td>\n",
       "      <td>2.584913</td>\n",
       "      <td>0.066445</td>\n",
       "      <td>0.494839</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.999928</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448505</td>\n",
       "      <td>0.362818</td>\n",
       "      <td>0.474876</td>\n",
       "      <td>0.384721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.484660</td>\n",
       "      <td>0.441929</td>\n",
       "      <td>0.484660</td>\n",
       "      <td>['RS']</td>\n",
       "      <td>RS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNORAE</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.019398</td>\n",
       "      <td>2.584913</td>\n",
       "      <td>0.066445</td>\n",
       "      <td>0.494839</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.999928</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448505</td>\n",
       "      <td>0.435348</td>\n",
       "      <td>0.447054</td>\n",
       "      <td>0.492395</td>\n",
       "      <td>0.450039</td>\n",
       "      <td>0.432771</td>\n",
       "      <td>0.413654</td>\n",
       "      <td>0.492395</td>\n",
       "      <td>['MMS']</td>\n",
       "      <td>MMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNORAU</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.019398</td>\n",
       "      <td>2.584913</td>\n",
       "      <td>0.066445</td>\n",
       "      <td>0.494839</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.999928</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448505</td>\n",
       "      <td>0.443713</td>\n",
       "      <td>0.468347</td>\n",
       "      <td>0.487077</td>\n",
       "      <td>0.432025</td>\n",
       "      <td>0.478051</td>\n",
       "      <td>0.505159</td>\n",
       "      <td>0.505159</td>\n",
       "      <td>['QT']</td>\n",
       "      <td>QT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>MLP</td>\n",
       "      <td>D300</td>\n",
       "      <td>0.019184</td>\n",
       "      <td>2.584899</td>\n",
       "      <td>0.066225</td>\n",
       "      <td>0.569462</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.986003</td>\n",
       "      <td>0.037949</td>\n",
       "      <td>0.250186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572848</td>\n",
       "      <td>0.601871</td>\n",
       "      <td>0.431757</td>\n",
       "      <td>0.601871</td>\n",
       "      <td>0.601871</td>\n",
       "      <td>0.438325</td>\n",
       "      <td>0.415073</td>\n",
       "      <td>0.601871</td>\n",
       "      <td>['NS' 'MMS' 'MAS']</td>\n",
       "      <td>NS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>OLA</td>\n",
       "      <td>D300</td>\n",
       "      <td>0.019184</td>\n",
       "      <td>2.584899</td>\n",
       "      <td>0.066225</td>\n",
       "      <td>0.569462</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.986003</td>\n",
       "      <td>0.037949</td>\n",
       "      <td>0.250186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572848</td>\n",
       "      <td>0.414099</td>\n",
       "      <td>0.445926</td>\n",
       "      <td>0.386136</td>\n",
       "      <td>0.446185</td>\n",
       "      <td>0.376316</td>\n",
       "      <td>0.497744</td>\n",
       "      <td>0.497744</td>\n",
       "      <td>['QT']</td>\n",
       "      <td>QT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>Percep</td>\n",
       "      <td>D300</td>\n",
       "      <td>0.019184</td>\n",
       "      <td>2.584899</td>\n",
       "      <td>0.066225</td>\n",
       "      <td>0.569462</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.986003</td>\n",
       "      <td>0.037949</td>\n",
       "      <td>0.250186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572848</td>\n",
       "      <td>0.265806</td>\n",
       "      <td>0.434663</td>\n",
       "      <td>0.237405</td>\n",
       "      <td>0.267137</td>\n",
       "      <td>0.377495</td>\n",
       "      <td>0.421451</td>\n",
       "      <td>0.434663</td>\n",
       "      <td>['SS']</td>\n",
       "      <td>SS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>SVM_RBF</td>\n",
       "      <td>D300</td>\n",
       "      <td>0.019184</td>\n",
       "      <td>2.584899</td>\n",
       "      <td>0.066225</td>\n",
       "      <td>0.569462</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.986003</td>\n",
       "      <td>0.037949</td>\n",
       "      <td>0.250186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572848</td>\n",
       "      <td>0.186589</td>\n",
       "      <td>0.190331</td>\n",
       "      <td>0.201437</td>\n",
       "      <td>0.179149</td>\n",
       "      <td>0.217621</td>\n",
       "      <td>0.202262</td>\n",
       "      <td>0.217621</td>\n",
       "      <td>['RS']</td>\n",
       "      <td>RS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>SVM_lin</td>\n",
       "      <td>D300</td>\n",
       "      <td>0.019184</td>\n",
       "      <td>2.584899</td>\n",
       "      <td>0.066225</td>\n",
       "      <td>0.569462</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.986003</td>\n",
       "      <td>0.037949</td>\n",
       "      <td>0.250186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572848</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.347853</td>\n",
       "      <td>0.121659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.326279</td>\n",
       "      <td>0.332856</td>\n",
       "      <td>0.347853</td>\n",
       "      <td>['SS']</td>\n",
       "      <td>SS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3600 rows × 144 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model Dataset  attr_conc.mean  attr_ent.mean  attr_to_inst  \\\n",
       "0     Bagging      D1        0.019398       2.584913      0.066445   \n",
       "1        GLVQ      D1        0.019398       2.584913      0.066445   \n",
       "2          GP      D1        0.019398       2.584913      0.066445   \n",
       "3      KNORAE      D1        0.019398       2.584913      0.066445   \n",
       "4      KNORAU      D1        0.019398       2.584913      0.066445   \n",
       "...       ...     ...             ...            ...           ...   \n",
       "3595      MLP    D300        0.019184       2.584899      0.066225   \n",
       "3596      OLA    D300        0.019184       2.584899      0.066225   \n",
       "3597   Percep    D300        0.019184       2.584899      0.066225   \n",
       "3598  SVM_RBF    D300        0.019184       2.584899      0.066225   \n",
       "3599  SVM_lin    D300        0.019184       2.584899      0.066225   \n",
       "\n",
       "      best_node.mean  best_node.mean.relative        c1        c2  \\\n",
       "0           0.494839                      5.0  0.999928  0.000199   \n",
       "1           0.494839                      5.0  0.999928  0.000199   \n",
       "2           0.494839                      5.0  0.999928  0.000199   \n",
       "3           0.494839                      5.0  0.999928  0.000199   \n",
       "4           0.494839                      5.0  0.999928  0.000199   \n",
       "...              ...                      ...       ...       ...   \n",
       "3595        0.569462                      6.5  0.986003  0.037949   \n",
       "3596        0.569462                      6.5  0.986003  0.037949   \n",
       "3597        0.569462                      6.5  0.986003  0.037949   \n",
       "3598        0.569462                      6.5  0.986003  0.037949   \n",
       "3599        0.569462                      6.5  0.986003  0.037949   \n",
       "\n",
       "      can_cor.mean  ...  linearity.class.L3_partial.1        NS        SS  \\\n",
       "0         0.202900  ...                      0.448505  0.440079  0.466025   \n",
       "1         0.202900  ...                      0.448505  0.434862  0.462094   \n",
       "2         0.202900  ...                      0.448505  0.362818  0.474876   \n",
       "3         0.202900  ...                      0.448505  0.435348  0.447054   \n",
       "4         0.202900  ...                      0.448505  0.443713  0.468347   \n",
       "...            ...  ...                           ...       ...       ...   \n",
       "3595      0.250186  ...                      0.572848  0.601871  0.431757   \n",
       "3596      0.250186  ...                      0.572848  0.414099  0.445926   \n",
       "3597      0.250186  ...                      0.572848  0.265806  0.434663   \n",
       "3598      0.250186  ...                      0.572848  0.186589  0.190331   \n",
       "3599      0.250186  ...                      0.572848  0.026667  0.347853   \n",
       "\n",
       "           MMS       MAS        RS        QT  Max_F1_perf            Best_STs  \\\n",
       "0     0.394689  0.303649  0.449671  0.451235     0.466025              ['SS']   \n",
       "1     0.418003  0.445430  0.469747  0.487364     0.487364              ['QT']   \n",
       "2     0.384721  0.000000  0.484660  0.441929     0.484660              ['RS']   \n",
       "3     0.492395  0.450039  0.432771  0.413654     0.492395             ['MMS']   \n",
       "4     0.487077  0.432025  0.478051  0.505159     0.505159              ['QT']   \n",
       "...        ...       ...       ...       ...          ...                 ...   \n",
       "3595  0.601871  0.601871  0.438325  0.415073     0.601871  ['NS' 'MMS' 'MAS']   \n",
       "3596  0.386136  0.446185  0.376316  0.497744     0.497744              ['QT']   \n",
       "3597  0.237405  0.267137  0.377495  0.421451     0.434663              ['SS']   \n",
       "3598  0.201437  0.179149  0.217621  0.202262     0.217621              ['RS']   \n",
       "3599  0.121659  0.000000  0.326279  0.332856     0.347853              ['SS']   \n",
       "\n",
       "      Best_ST  \n",
       "0          SS  \n",
       "1          QT  \n",
       "2          RS  \n",
       "3         MMS  \n",
       "4          QT  \n",
       "...       ...  \n",
       "3595       NS  \n",
       "3596       QT  \n",
       "3597       SS  \n",
       "3598       RS  \n",
       "3599       SS  \n",
       "\n",
       "[3600 rows x 144 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5b4563a-0102-4987-bd7c-633bdfaf4c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best_ST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NS</th>\n",
       "      <td>1107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAS</th>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RS</th>\n",
       "      <td>568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SS</th>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QT</th>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMS</th>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Best_ST\n",
       "NS      1107\n",
       "MAS      610\n",
       "RS       568\n",
       "SS       461\n",
       "QT       440\n",
       "MMS      414"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dataset['Best_ST'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18b8914d-1d28-4856-811e-03645befefc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAADFCAYAAABafUR1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAi/klEQVR4nO3de1RU9f7/8dcIgqJcVISBrwh+OV5CMStLOallXiBZfb2wOt4ITI997WBZXio9rrQ0UVxaWSbn+42LnjKts9IMO6WoJ0tN01K8dEzxm+gXBjxemNCvyGV+f7icX5OXdByYkf18rLXXYn8+H/Z+b3fbebX97D0mm81mEwAAAGAQjdxdAAAAAFCfCMAAAAAwFAIwAAAADIUADAAAAEMhAAMAAMBQCMAAAAAwFAIwAAAADMXb3QXcCWpra1VcXCx/f3+ZTCZ3lwMAAIBfsdls+vnnnxUeHq5GjW58j5cAfBOKi4sVERHh7jIAAADwG06cOKE2bdrccAwB+Cb4+/tLuvwHGhAQ4OZqAAAA8GtWq1URERH23HYjBOCbcGXaQ0BAAAEYAADAg93MdFUeggMAAIChEIABAABgKARgAAAAGAoBGAAAAIZCAAYAAICh8BYIDxP10np3l+BSP81PdHcJAAAADrgDDAAAAEMhAAMAAMBQCMAAAAAwFAIwAAAADIUADAAAAEMhAAMAAMBQCMAAAAAwFAIwAAAADIUADAAAAENxawDeunWrHnvsMYWHh8tkMmnt2rUO/TabTS+//LLCwsLUtGlT9e/fX0eOHHEYc+bMGY0ePVoBAQEKCgrSuHHjVFFR4TCmoKBAvXv3VpMmTRQREaGMjIy6PjQAAAB4KLcG4PPnz+vuu+/W0qVLr9mfkZGhJUuWKDMzUzt37lSzZs0UHx+vixcv2seMHj1aBw8e1MaNG5WXl6etW7fqqaeesvdbrVYNHDhQkZGR2rNnjxYuXKjZs2frv/7rv+r8+AAAAOB5TDabzebuIiTJZDJpzZo1GjJkiKTLd3/Dw8M1ZcoUTZ06VZJUXl6u0NBQ5ebmasSIEfrhhx8UExOjb7/9Vt27d5ckff755xo0aJBOnjyp8PBwLVu2TH/+859lsVjk4+MjSXrppZe0du1a/fOf/7yp2qxWqwIDA1VeXq6AgADXH/wvRL20vk63X99+mp/o7hIAAIAB3Epe89g5wP/zP/8ji8Wi/v3729sCAwPVo0cP7dixQ5K0Y8cOBQUF2cOvJPXv31+NGjXSzp077WP69OljD7+SFB8fr8OHD+vs2bPX3HdlZaWsVqvDAgAAgIbBYwOwxWKRJIWGhjq0h4aG2vssFotCQkIc+r29vdWyZUuHMdfaxi/38Wvp6ekKDAy0LxEREbd/QAAAAPAIHhuA3Wn69OkqLy+3LydOnHB3SQAAAHARjw3AZrNZklRaWurQXlpaau8zm80qKytz6K+urtaZM2ccxlxrG7/cx6/5+voqICDAYQEAAEDD4LEBuF27djKbzdq0aZO9zWq1aufOnYqLi5MkxcXF6dy5c9qzZ499zObNm1VbW6sePXrYx2zdulVVVVX2MRs3blTHjh3VokWLejoaAAAAeAq3BuCKigrt3btXe/fulXT5wbe9e/eqqKhIJpNJzz33nObOnat169Zp//79SklJUXh4uP1NEXfddZcSEhI0fvx47dq1S9u2bdPEiRM1YsQIhYeHS5JGjRolHx8fjRs3TgcPHtTq1av15ptvavLkyW46agAAALiTtzt3vnv3bvXt29e+fiWUpqamKjc3Vy+88ILOnz+vp556SufOnVOvXr30+eefq0mTJvbfef/99zVx4kT169dPjRo1UlJSkpYsWWLvDwwM1IYNG5SWlqb77rtPwcHBevnllx3eFQwAAADj8Jj3AHsy3gPsPN4DDAAA6kODeA8wAAAAUBcIwAAAADAUAjAAAAAMhQAMAAAAQyEAAwAAwFAIwAAAADAUAjAAAAAMhQAMAAAAQyEAAwAAwFAIwAAAADAUAjAAAAAMhQAMAAAAQyEAAwAAwFAIwAAAADAUAjAAAAAMhQAMAAAAQ3EqAB87dszVdQAAAAD1wqkA/Lvf/U59+/bVe++9p4sXL7q6JgAAAKDOOBWAv/vuO3Xt2lWTJ0+W2WzWf/7nf2rXrl2urg0AAABwOacCcLdu3fTmm2+quLhY2dnZKikpUa9evdSlSxctXrxYp06dcnWdAAAAgEvc1kNw3t7eGjZsmD766CMtWLBAR48e1dSpUxUREaGUlBSVlJS4qk4AAADAJW4rAO/evVt/+tOfFBYWpsWLF2vq1KkqLCzUxo0bVVxcrMGDB7uqTgAAAMAlnArAixcvVmxsrH7/+9+ruLhYK1as0PHjxzV37ly1a9dOvXv3Vm5urr777rvbLjAqKkomk+mqJS0tTZL08MMPX9U3YcIEh20UFRUpMTFRfn5+CgkJ0bRp01RdXX3btQEAAODO4+3MLy1btkxjx47VmDFjFBYWds0xISEhysrKuq3iJOnbb79VTU2Nff3AgQMaMGCAHn/8cXvb+PHj9eqrr9rX/fz87D/X1NQoMTFRZrNZ27dvV0lJiVJSUtS4cWPNmzfvtusDAADAncWpAHzkyJHfHOPj46PU1FRnNu+gdevWDuvz589XdHS0HnroIXubn5+fzGbzNX9/w4YNOnTokPLz8xUaGqpu3bppzpw5evHFFzV79mz5+Pjcdo0AAAC4czg1BSInJ0cfffTRVe0fffSRli9ffttFXc+lS5f03nvvaezYsTKZTPb2999/X8HBwerSpYumT5+uCxcu2Pt27Nih2NhYhYaG2tvi4+NltVp18ODBa+6nsrJSVqvVYQEAAEDD4FQATk9PV3Bw8FXtISEhdTqtYO3atTp37pzGjBljbxs1apTee+89bdmyRdOnT9df//pXJScn2/stFotD+JVkX7dYLNfcT3p6ugIDA+1LRESE6w8GAAAAbuHUFIiioiK1a9fuqvbIyEgVFRXddlHXk5WVpUcffVTh4eH2tqeeesr+c2xsrMLCwtSvXz8VFhYqOjraqf1Mnz5dkydPtq9brVZCMAAAQAPh1B3gkJAQFRQUXNW+b98+tWrV6raLupbjx48rPz9ff/zjH284rkePHpKko0ePSpLMZrNKS0sdxlxZv968YV9fXwUEBDgsAAAAaBicCsAjR47Us88+qy1btqimpkY1NTXavHmzJk2apBEjRri6RkmX5x2HhIQoMTHxhuP27t0rSfa3U8TFxWn//v0qKyuzj9m4caMCAgIUExNTJ7UCAADAczk1BWLOnDn66aef1K9fP3l7X95EbW2tUlJS6mQOcG1trXJycpSammrfnyQVFhZq5cqVGjRokFq1aqWCggI9//zz6tOnj7p27SpJGjhwoGJiYvTEE08oIyNDFotFM2fOVFpamnx9fV1eKwAAADybUwHYx8dHq1ev1pw5c7Rv3z41bdpUsbGxioyMdHV9kqT8/HwVFRVp7NixV9WRn5+vN954Q+fPn1dERISSkpI0c+ZM+xgvLy/l5eXp6aefVlxcnJo1a6bU1FSH9wYDAADAOEw2m83m7iI8ndVqVWBgoMrLy+t8PnDUS+vrdPv17af5N56yAgAA4Aq3ktecugNcU1Oj3Nxcbdq0SWVlZaqtrXXo37x5szObBQAAAOqcUwF40qRJys3NVWJiorp06eLwpRQAAACAJ3MqAK9atUoffvihBg0a5Op6AAAAgDrl1GvQfHx89Lvf/c7VtQAAAAB1zqkAPGXKFL355pvi+TkAAADcaZyaAvH1119ry5Yt+vvf/67OnTurcePGDv0ff/yxS4oDAAAAXM2pABwUFKShQ4e6uhYAAACgzjkVgHNyclxdBwAAAFAvnJoDLEnV1dXKz8/XX/7yF/3888+SpOLiYlVUVLisOAAAAMDVnLoDfPz4cSUkJKioqEiVlZUaMGCA/P39tWDBAlVWViozM9PVdQIAAAAu4dQd4EmTJql79+46e/asmjZtam8fOnSoNm3a5LLiAAAAAFdz6g7wV199pe3bt8vHx8ehPSoqSv/7v//rksIATxT10np3l+AyP81PdHcJAAC4hVN3gGtra1VTU3NV+8mTJ+Xv73/bRQEAAAB1xakAPHDgQL3xxhv2dZPJpIqKCs2aNYuvRwYAAIBHc2oKxKJFixQfH6+YmBhdvHhRo0aN0pEjRxQcHKwPPvjA1TUCAAAALuNUAG7Tpo327dunVatWqaCgQBUVFRo3bpxGjx7t8FAcAAAA4GmcCsCS5O3treTkZFfWAgAAANQ5pwLwihUrbtifkpLiVDEA4KyG9IYOibd0AEBdcioAT5o0yWG9qqpKFy5ckI+Pj/z8/AjAAAAA8FhOvQXi7NmzDktFRYUOHz6sXr168RAcAAAAPJpTAfha2rdvr/nz5191dxgAAADwJC4LwNLlB+OKi4tduUkAAADApZwKwOvWrXNYPvnkE2VmZio5OVkPPvigy4qbPXu2TCaTw9KpUyd7/8WLF5WWlqZWrVqpefPmSkpKUmlpqcM2ioqKlJiYKD8/P4WEhGjatGmqrq52WY0AAAC4szj1ENyQIUMc1k0mk1q3bq1HHnlEixYtckVddp07d1Z+fr593dv7/5f8/PPPa/369froo48UGBioiRMnatiwYdq2bZskqaamRomJiTKbzdq+fbtKSkqUkpKixo0ba968eS6tEwAAAHcGpwJwbW2tq+u4Lm9vb5nN5qvay8vLlZWVpZUrV+qRRx6RJOXk5Oiuu+7SN998o549e2rDhg06dOiQ8vPzFRoaqm7dumnOnDl68cUXNXv2bPn4+NTbcQAAAMAzuHQOcF04cuSIwsPD9e///u8aPXq0ioqKJEl79uxRVVWV+vfvbx/bqVMntW3bVjt27JAk7dixQ7GxsQoNDbWPiY+Pl9Vq1cGDB6+7z8rKSlmtVocFAAAADYNTd4AnT55802MXL17szC4kST169FBubq46duyokpISvfLKK+rdu7cOHDggi8UiHx8fBQUFOfxOaGioLBaLJMlisTiE3yv9V/quJz09Xa+88orTdQMAAMBzORWAv//+e33//feqqqpSx44dJUk//vijvLy8dO+999rHmUym2yru0Ucftf/ctWtX9ejRQ5GRkfrwww/VtGnT29r2jUyfPt0h5FutVkVERNTZ/gAAAFB/nArAjz32mPz9/bV8+XK1aNFC0uUvx3jyySfVu3dvTZkyxaVFXhEUFKQOHTro6NGjGjBggC5duqRz58453AUuLS21zxk2m83atWuXwzauvCXiWvOKr/D19ZWvr6/rDwAAAABu59Qc4EWLFik9Pd0efiWpRYsWmjt3rsvfAvFLFRUVKiwsVFhYmO677z41btxYmzZtsvcfPnxYRUVFiouLkyTFxcVp//79Kisrs4/ZuHGjAgICFBMTU2d1AgAAwHM5dQfYarXq1KlTV7WfOnVKP//8820XdcXUqVP12GOPKTIyUsXFxZo1a5a8vLw0cuRIBQYGaty4cZo8ebJatmypgIAAPfPMM4qLi1PPnj0lSQMHDlRMTIyeeOIJZWRkyGKxaObMmUpLS+MOLwAAgEE5FYCHDh2qJ598UosWLdIDDzwgSdq5c6emTZumYcOGuay4kydPauTIkTp9+rRat26tXr166ZtvvlHr1q0lSa+//roaNWqkpKQkVVZWKj4+Xu+884799728vJSXl6enn35acXFxatasmVJTU/Xqq6+6rEYAwI1FvbTe3SW41E/zE91dAoDb5FQAzszM1NSpUzVq1ChVVVVd3pC3t8aNG6eFCxe6rLhVq1bdsL9JkyZaunSpli5det0xkZGR+uyzz1xWEwAAAO5sTgVgPz8/vfPOO1q4cKEKCwslSdHR0WrWrJlLiwMAAABczakAfEVJSYlKSkrUp08fNW3aVDab7bZffQYAAOpPQ5qiwvQU3Cyn3gJx+vRp9evXTx06dNCgQYNUUlIiSRo3blydvQINAAAAcAWnAvDzzz+vxo0bq6ioSH5+fvb24cOH6/PPP3dZcQAAAICrOTUFYsOGDfriiy/Upk0bh/b27dvr+PHjLikMAADAyBrS9BTJs6aoOHUH+Pz58w53fq84c+YM79cFAACAR3MqAPfu3VsrVqywr5tMJtXW1iojI0N9+/Z1WXEAAACAqzk1BSIjI0P9+vXT7t27denSJb3wwgs6ePCgzpw5o23btrm6RgAAAMBlnLoD3KVLF/3444/q1auXBg8erPPnz2vYsGH6/vvvFR0d7eoaAQAAAJe55TvAVVVVSkhIUGZmpv785z/XRU0AAABAnbnlO8CNGzdWQUFBXdQCAAAA1DmnpkAkJycrKyvL1bUAAAAAdc6ph+Cqq6uVnZ2t/Px83XfffWrWrJlD/+LFi11SHAAAAOBqtxSAjx07pqioKB04cED33nuvJOnHH390GGMymVxXHQAAAOBitxSA27dvr5KSEm3ZskXS5a8+XrJkiUJDQ+ukOAAAAMDVbmkOsM1mc1j/+9//rvPnz7u0IAAAAKAuOfUQ3BW/DsQAAACAp7ulAGwyma6a48ucXwAAANxJbmkOsM1m05gxY+Tr6ytJunjxoiZMmHDVWyA+/vhj11UIAAAAuNAtBeDU1FSH9eTkZJcWAwAAANS1WwrAOTk5dVUHAAAAUC9u6yE4AAAA4E7j0QE4PT1d999/v/z9/RUSEqIhQ4bo8OHDDmMefvhh+8N5V5YJEyY4jCkqKlJiYqL8/PwUEhKiadOmqbq6uj4PBQAAAB7Cqa9Cri9ffvml0tLSdP/996u6ulozZszQwIEDdejQIYcH78aPH69XX33Vvu7n52f/uaamRomJiTKbzdq+fbtKSkqUkpKixo0ba968efV6PAAAAHA/jw7An3/+ucN6bm6uQkJCtGfPHvXp08fe7ufnJ7PZfM1tbNiwQYcOHVJ+fr5CQ0PVrVs3zZkzRy+++KJmz54tHx+fOj0GAAAAeBaPngLxa+Xl5ZKkli1bOrS///77Cg4OVpcuXTR9+nRduHDB3rdjxw7FxsY6fF1zfHy8rFarDh48eM39VFZWymq1OiwAAABoGDz6DvAv1dbW6rnnntODDz6oLl262NtHjRqlyMhIhYeHq6CgQC+++KIOHz5sfxexxWJxCL+S7OsWi+Wa+0pPT9crr7xSR0cCAAAAd7pjAnBaWpoOHDigr7/+2qH9qaeesv8cGxursLAw9evXT4WFhYqOjnZqX9OnT9fkyZPt61arVREREc4VDgAAAI9yR0yBmDhxovLy8rRlyxa1adPmhmN79OghSTp69KgkyWw2q7S01GHMlfXrzRv29fVVQECAwwIAAICGwaMDsM1m08SJE7VmzRpt3rxZ7dq1+83f2bt3ryQpLCxMkhQXF6f9+/errKzMPmbjxo0KCAhQTExMndQNAAAAz+XRUyDS0tK0cuVKffLJJ/L397fP2Q0MDFTTpk1VWFiolStXatCgQWrVqpUKCgr0/PPPq0+fPurataskaeDAgYqJidETTzyhjIwMWSwWzZw5U2lpafL19XXn4QEAAMANPPoO8LJly1ReXq6HH35YYWFh9mX16tWSJB8fH+Xn52vgwIHq1KmTpkyZoqSkJH366af2bXh5eSkvL09eXl6Ki4tTcnKyUlJSHN4bDAAAAOPw6DvANpvthv0RERH68ssvf3M7kZGR+uyzz1xVFgAAAO5gHn0HGAAAAHA1AjAAAAAMhQAMAAAAQyEAAwAAwFAIwAAAADAUAjAAAAAMhQAMAAAAQyEAAwAAwFAIwAAAADAUAjAAAAAMhQAMAAAAQyEAAwAAwFAIwAAAADAUAjAAAAAMhQAMAAAAQyEAAwAAwFAIwAAAADAUAjAAAAAMhQAMAAAAQyEAAwAAwFAIwAAAADAUAjAAAAAMhQAMAAAAQzFUAF66dKmioqLUpEkT9ejRQ7t27XJ3SQAAAKhnhgnAq1ev1uTJkzVr1ix99913uvvuuxUfH6+ysjJ3lwYAAIB65O3uAurL4sWLNX78eD355JOSpMzMTK1fv17Z2dl66aWXHMZWVlaqsrLSvl5eXi5JslqtdV5nbeWFOt9HfaqPP7P61JDOD+fGszWk88O58WwN6fxwbjxbXZ+fK9u32Wy/OdZku5lRd7hLly7Jz89Pf/vb3zRkyBB7e2pqqs6dO6dPPvnEYfzs2bP1yiuv1HOVAAAAuF0nTpxQmzZtbjjGEHeA//Wvf6mmpkahoaEO7aGhofrnP/951fjp06dr8uTJ9vXa2lqdOXNGrVq1kslkqvN665rValVERIROnDihgIAAd5eDX+DceDbOj+fi3Hguzo1na0jnx2az6eeff1Z4ePhvjjVEAL5Vvr6+8vX1dWgLCgpyTzF1KCAg4I7/j72h4tx4Ns6P5+LceC7OjWdrKOcnMDDwpsYZ4iG44OBgeXl5qbS01KG9tLRUZrPZTVUBAADAHQwRgH18fHTfffdp06ZN9rba2lpt2rRJcXFxbqwMAAAA9c0wUyAmT56s1NRUde/eXQ888IDeeOMNnT9/3v5WCCPx9fXVrFmzrprmAffj3Hg2zo/n4tx4Ls6NZzPq+THEWyCuePvtt7Vw4UJZLBZ169ZNS5YsUY8ePdxdFgAAAOqRoQIwAAAAYIg5wAAAAMAVBGAAAAAYCgEYAAAAhkIABgAAgKEQgBuoMWPGyGQyaf78+Q7ta9eudfg65//+7//W3XffrebNmysoKEj33HOP0tPT67vcBuvKeZgwYcJVfWlpaTKZTBozZoxD+44dO+Tl5aXExMRrbnPNmjXq2bOnAgMD5e/vr86dO+u5556rg+qN6co5M5lMaty4sdq1a6cXXnhBFy9etI/58ssv9cgjj6hly5by8/NT+/btlZqaqkuXLrmxcmM4deqUnn76abVt21a+vr4ym82Kj4/Xtm3bJEn79u3Tf/zHfygkJERNmjRRVFSUhg8frrKyMjdX3vCdOHFCY8eOVXh4uHx8fBQZGalJkybp9OnT+umnn+zX1fWW3Nxcdx/CHe1WPm9u9bPpt667OxEBuAFr0qSJFixYoLNnz16zPzs7W88995yeffZZ7d27V9u2bdMLL7ygioqKeq60YYuIiNCqVav0f//3f/a2ixcvauXKlWrbtu1V47OysvTMM89o69atKi4udujbtGmThg8frqSkJO3atUt79uzRa6+9pqqqqjo/DiNJSEhQSUmJjh07ptdff11/+ctfNGvWLEnSoUOHlJCQoO7du2vr1q3av3+/3nrrLfn4+KimpsbNlTd8SUlJ+v7777V8+XL9+OOPWrdunR5++GGdPn1ap06dUr9+/dSyZUt98cUX+uGHH5STk6Pw8HCdP3/e3aU3aMeOHVP37t115MgRffDBBzp69KgyMzPtXzjl7++vkpIS+zJlyhR17tzZoW348OHuPow73q183tzK2Btdd3cqw3wRhhH1799fR48eVXp6ujIyMq7qX7dunf7whz9o3Lhx9rbOnTvXZ4mGcO+996qwsFAff/yxRo8eLUn6+OOP1bZtW7Vr185hbEVFhVavXq3du3fLYrEoNzdXM2bMsPd/+umnevDBBzVt2jR7W4cOHTRkyJB6ORajuHKHQ7r8IdG/f39t3LhRCxYs0IYNG2Q2mx2uqejoaCUkJLirXMM4d+6cvvrqK/3jH//QQw89JEmKjIzUAw88IOnyv3CVl5fr3Xfflbf35Y+3du3aqW/fvm6r2SjS0tLk4+OjDRs2qGnTppKktm3b6p577lF0dLRmzpypZcuW2cc3b95c3t7e9usMrnErnzc3O/a3rrs7FXeAGzAvLy/NmzdPb731lk6ePHlVv9ls1jfffKPjx4+7oTpjGTt2rHJycuzr2dnZ1/wWwg8//FCdOnVSx44dlZycrOzsbP3yVd1ms1kHDx7UgQMH6qVuSAcOHND27dvl4+Mj6fI5KCkp0datW91cmfE0b95czZs319q1a1VZWXlVv9lsVnV1tdasWSNecV9/zpw5oy+++EJ/+tOf7OH3CrPZrNGjR2v16tWck3pys583Nzv2t667OxUBuIEbOnSounXrZv/n21+aNWuWgoKCFBUVpY4dO2rMmDH68MMPVVtb64ZKG7bk5GR9/fXXOn78uI4fP65t27YpOTn5qnFZWVn29oSEBJWXl+vLL7+09z/zzDO6//77FRsbq6ioKI0YMULZ2dkN6i8lT5CXl6fmzZurSZMmio2NVVlZmf2u++OPP66RI0fqoYceUlhYmIYOHaq3335bVqvVzVU3fN7e3srNzdXy5csVFBSkBx98UDNmzFBBQYEkqWfPnpoxY4ZGjRql4OBgPfroo1q4cKFKS0vdXHnDduTIEdlsNt11113X7L/rrrt09uxZnTp1qp4rM6ab/by52bG/dd3dqQjABrBgwQItX75cP/zwg0N7WFiYduzYof3792vSpEmqrq5WamqqEhISCMEu1rp1ayUmJio3N1c5OTlKTExUcHCww5jDhw9r165dGjlypKTLf+kMHz5cWVlZ9jHNmjXT+vXrdfToUc2cOVPNmzfXlClT9MADD+jChQv1ekwNWd++fbV3717t3LlTqampevLJJ5WUlCTp8r+s5OTk6OTJk8rIyNC//du/ad68efb5jKhbSUlJKi4u1rp165SQkKB//OMfuvfee+0PUL322muyWCzKzMxU586dlZmZqU6dOmn//v3uLdwAfusO75V/RUHdupnPm1sd+1vX3R3JhgYpNTXVNnjwYPv6oEGDbIMHD7atWbPGdqPT/tVXX9kk2TZv3lwPVTZ8vzwPeXl5tqioKFtUVJRt/fr1NpvNZhs8eLAtNTXVZrPZbNOmTbNJsnl5edmXRo0a2Zo2bWo7d+7cdfdx7Ngxm7e3ty07O7uuD8cQfn3t1NTU2Lp06WJ79913r/s7Z86csQUHB9tefvnleqgQvzZu3Dhb27Ztr9lXWVlpi4mJsaWkpNRzVcbxr3/9y2YymWyvvfbaNfvHjx9va926tUPbrFmzbHfffXc9VGcct/J5cytjr+dG192dgDvABjF//nx9+umn2rFjxw3HxcTESBJPTNeBhIQEXbp0SVVVVYqPj3foq66u1ooVK7Ro0SLt3bvXvuzbt0/h4eH64IMPrrvdqKgo+fn5cc7qSKNGjTRjxgzNnDnT4WnpX2rRooXCwsI4B24SExNz3T97Hx8fRUdHc27qUKtWrTRgwAC98847V10jFotF77///lWve0TdutHnze2M/aUbXXd3At4CYRCxsbEaPXq0lixZYm97+umnFR4erkceeURt2rRRSUmJ5s6dq9atWysuLs6N1TZMXl5e9mkoXl5eDn15eXk6e/asxo0bp8DAQIe+pKQkZWVlacKECZo9e7YuXLigQYMGKTIyUufOndOSJUtUVVWlAQMG1NuxGM3jjz+uadOmaenSpfL399fevXs1dOhQRUdH6+LFi1qxYoUOHjyot956y92lNminT5/W448/rrFjx6pr167y9/fX7t27lZGRocGDBysvL0+rVq3SiBEj1KFDB9lsNn366af67LPPHB70geu9/fbb+v3vf6/4+HjNnTtX7dq108GDBzVt2jR16NBBL7/8srtLNJQbfd7c6tjfuu7uVARgA3n11Ve1evVq+3r//v2VnZ2tZcuW6fTp0woODlZcXJw2bdqkVq1aubHShisgIOCa7VlZWerfv/9V4Ve6HIAzMjJUUFCghx56SEuXLlVKSopKS0vVokUL3XPPPdqwYYM6duxY1+Ublre3tyZOnKiMjAytWbNGX3/9tSZMmKDi4mI1b95cnTt31tq1a+2vCELdaN68uXr06KHXX39dhYWFqqqqUkREhMaPH68ZM2aopKREfn5+mjJlik6cOCFfX1+1b99e7777rp544gl3l9+gtW/fXt9++61mz56tP/zhDyorK5PNZtOwYcP017/+VX5+fu4u0XCu93lzq2N/67q7U5lsNt5LAgAAXGvWrFlavHixNm7cqJ49e7q7HMABARgAANSJnJwclZeX69lnn1WjRjx2BM9BAAYAAICh8L9jAAAAMBQCMAAAAAyFAAwAAABDIQADAADAUAjAAAAAMBQCMAAAAAyFAAwAAABDIQADAADAUAjAAAAAMJT/B9hUivafdvD1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 800x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['Best_ST'].value_counts().plot.bar(stacked='True', rot=0, ylabel='Frequency')\n",
    "plt.gcf().set_figwidth(8)\n",
    "plt.gcf().set_figheight(2)\n",
    "plt.savefig('../figs/class_freq.pdf', bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60903323-fd67-4537-9226-5abaaea253ae",
   "metadata": {},
   "source": [
    "Dataset is imbalanced, this will be tackled within the classification algorithm using class_weight='balanced'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8879947f-21f3-4c08-8b48-f2d3d2b05788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to store computing times:\n",
    "computing_times = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaea31c-6fe4-4157-ad63-a4d805535358",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Running the experiment with LOOCV. Meta-model: RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4274194f-f8bd-4a6e-9671-546f5badf181",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training metamodel for 'Model' Bagging.\n",
      "---+++--+-+-+----+-----+--+--+-+-----++++---+--++-----+-+++-+++--++--+--+-+-+++-+--++----+++----+++++-+-------++-++-++--++--+--+-+++++--++++-+++-+-----+-++-+--+----------------++--++-+--++---+++++++-+--+------+---++-+--+--+-----+-++--+------+++-+---+++-+-+---+--------+++--+-+---++------------------+\n",
      "Training metamodel for 'Model' GLVQ.\n",
      "--+--++++++++--+-+-+-+-+--++-+-++++++++++-+-++--+-+--+-+++++++++-++-++++++--++-++---+-++--+++++++++++-++++++++-++++++++++-+-++-++++++++++++++++++++-++-++++++++++++++-+-++++--+++++++++-+++++++++++++++++++++++--++++--+++++++++++++++-+++++++++-++++++--++++++++++++++++++-++++++++-++++++++++++-+++++-+-++\n",
      "Training metamodel for 'Model' GP.\n",
      "-----+++++++-+++++++++--+----+-+-+++-+++++-+++-+++---++++++-+++--++--------+++-+--+++++-+-+++-+--++++---++++-+--++++++-+-+--++++--+++-+--+++--+--+---+-+++++-++++-+----+----+---++++++--+-+-+-+++++++-++-------+++-----+---+++++-++++++--+++--++-+--+++-+++-++++-++++-+--+++-----------++-----+++++-+-----+-\n",
      "Training metamodel for 'Model' KNORAE.\n",
      "--++-++++++-++++++++-----+++---+++-+++++-+++-+---+--+++-+----+---++--+-+--+----+---+---++++--+---++++---++-++--++-+---+--+-+----+------+-----------+---++--+++--+----------++----------+--------+---+------+----+---+--+-----+-----+-------------+---+---++--------+--+--+-----++------++++---+++++-+-++--++\n",
      "Training metamodel for 'Model' KNORAU.\n",
      "-----++++++++++-+------++-------+----+++++-++--+----+---+++-+----++--+---++--+++-------+-+++----++--+---------+++++--++-----+---++++++++-+++++++-+-+-+-+-++-++-+--+-----+-+-----+++-++-----+--+++-+++--+-+-----------+-------+-----+--+---+-+-+--+---+---++--+-----+------++++---------++--------+-+-++-----\n",
      "Training metamodel for 'Model' LCA.\n",
      "---+----++-------++---+--+---------------+++--+---+-----+---++-++-------+++--+++--+--------+-+-+--++--++-----+--++-----+--+-+-----+--+-+-+---++----++---+---+--+----------------++---+--+++---+-+---------------+---------------+++-------+-+--+-+---+---++--+-+---+--+--+-------------++---------+-----+---\n",
      "Training metamodel for 'Model' MCB.\n",
      "+-+--+++++++-++-+-+++-+--++----++----+++++-+--++-----+-+-++-+++--++--+-+-++---++-++++-+-++-++-++--+--+-+--++---+++-+-++--------------+--++-+-----++---+++-++---++----+++---+--+---++-+------+--+-+--------------+----++------+----+-+-+------++--+--++---++--+----++----------+--++--+-+++++---++++------++-\n",
      "Training metamodel for 'Model' MLP.\n",
      "+++++++++++-+---++++++++++++-++++--+++++++-++-+--+-----++-+-+-+--++++--+--+---+++-----+-+++++-+---++++--+++--++++++-++--++---++++-++-----+++---+-+-----++++++++++-----+-----++--++--++---++---+++---+++++++++++--++-+------+-+----++---+++++++++-++--++-++++++-+---+--++-++--+---+++---++++++-+++++--+-+++++\n",
      "Training metamodel for 'Model' OLA.\n",
      "---+-++-++++--+--++++-+--++-+--++----+++++-++--+--+--++--++-++++--+-----+-+--+-+----+-----+---+---+----+--++--++++++-++----------+----------+--++++---++-+--+---+-+------+----++-------------+-+-----+----+---+-+-++-----++----+----+---+-+----+-+---++--++-+-----+-+------+-----------++++---+++++------+++\n",
      "Training metamodel for 'Model' Percep.\n",
      "----+--------++------------------++---+---+---------------+-----+---+--------+-----------++-+-+-+------+------------+-----+----------------+-++--+------+---+--+----+------+----------+---+-----------------+-+---+--------+--+--++-----+--------+--++---++----+----+--------++------+++++-+++-+---+-++-----\n",
      "Training metamodel for 'Model' SVM_RBF.\n",
      "-----++--++++++++++++-----+----+++++-+++-+++++-+++-++--++++++++++++--++++++-+++++---+++-+-+++++++++++-++++-+---++++-++-----++-----++++++++++--++-++-+++++++-+-+++--+--+------++-+++++++++++++--+++++++--++++-+--+-----++-++-+-+++++-++-+++++++++-+---+---++++++-+++++++--++-------+---+++-----++++++-++---+-\n",
      "Training metamodel for 'Model' SVM_lin.\n",
      "-+++++++++++++-++-++-+-+++---+-++----++++-+--+++++---+--+++-+++--++--+---++-++-+-+--+--++-++-+--+++++--+++---+-++++-++-+--++---+--+++----+++++-+-+---+-+++--+-+++++++-++++--+---++--++--+-+++--+++++++++++++++---+-+-+--+++--+-++++-+-++++++++++++++-++--++--+-+-+-++++--++----++---+-+++---+--++---------+-\n",
      "Training time: 643.9853849117644 seconds.\n",
      "Testing time: 78.7351495877374 seconds.\n",
      "CPU times: user 4h 30min 16s, sys: 8min 3s, total: 4h 38min 19s\n",
      "Wall time: 14min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "training_time = 0\n",
    "testing_time = 0\n",
    "# Creating DataFrames to store results:\n",
    "summary_of_predictions_dict = {'Model':[], 'Dataset':[], 'Oracle\\'s ST':[],\n",
    "                               'Oracles\\'s Perf':[], 'Meta-scaler\\'s ST':[], \n",
    "                               'Meta-scaler\\'s Perf':[]}\n",
    "\n",
    "metamodel_performances_dict = {'Model':[], 'Accuracy':[], 'F1': []}\n",
    "models_names = dataset['Model'].unique()\n",
    "feat_importances_dict = dict(zip(models_names, [[]]*len(models_names)))\n",
    "\n",
    "#print(f'Shape of original X: {dataset.iloc[:,2:-9].shape}.')\n",
    "for model in models_names: # Running for each of the 12 models\n",
    "    print(f'\\nTraining metamodel for \\'Model\\' {model}.')\n",
    "    df = dataset[dataset['Model'] == model]\n",
    "    df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "    # Separating X and y.\n",
    "    X = df.iloc[:, 2:-9] # Just the metafeatures.\n",
    "    y = df.iloc[:,-1] # Just the best ST.\n",
    "    \n",
    "\n",
    "    # Splitting the dataset into the Training set and Test set folds, according to Leave One Out cross validation:\n",
    "    loo = LeaveOneOut()\n",
    "    feat_import_per_fold = []\n",
    "    y_true, y_pred = list(), list()\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        #print('.', end='')\n",
    "        # Separating training and test sets:\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # Filling missing values with a KNN imputer.  Each sample’s missing values are imputed\n",
    "        # using the mean value from n_neighbors nearest neighbors found in the training set. \n",
    "        imputer = KNNImputer(n_neighbors=2)\n",
    "        X_train = imputer.fit_transform(X_train)\n",
    "        X_test = imputer.transform(X_test)\n",
    "\n",
    "        # The above method returns a nd.array, so here we rebuild the DataFrame:\n",
    "        #X_train = pd.DataFrame(X_train, columns=X.columns) \n",
    "\n",
    "        # # Feature Scaling (Not needed if using a tree based meta-model)\n",
    "        # sc = MinMaxScaler()\n",
    "        # X_train = sc.fit_transform(X_train)\n",
    "        # X_test = sc.transform(X_test)\n",
    "\n",
    "        # # Feature Selection\n",
    "        # selector = SelectKBest(chi2, k=100).fit(X_train, y_train)\n",
    "        # X_train = selector.transform(X_train)\n",
    "        # X_test = selector.transform(X_test)\n",
    "        # #print(f'Model: {model}. Shape of transformed X_train: {X_train.shape}')\n",
    "        \n",
    "        # Fitting the metamodel.\n",
    "        # Using class weights to mitigate the effects of class imbalance.\n",
    "        \n",
    "        tic = time.perf_counter()\n",
    "        metaclf = RandomForestClassifier(class_weight='balanced', n_estimators=100, \n",
    "                                         n_jobs=-1, random_state=13)\n",
    "        metaclf.fit(X_train, y_train)\n",
    "        toc = time.perf_counter()\n",
    "        training_time += toc-tic\n",
    "        \n",
    "        # Evaluate model\n",
    "        tic = time.perf_counter()\n",
    "        y_hat = metaclf.predict(X_test)\n",
    "        toc = time.perf_counter()\n",
    "        testing_time += toc-tic\n",
    "        \n",
    "        # Printing a '+' if instance was correctly classified, else prints '-':\n",
    "        #print(f'Test instance {test_index}: {df.iloc[test_index,:].Dataset.values[0]}', end='')\n",
    "        if y_hat[0] == y_test.values[0]: print('+', end='')\n",
    "        else: print('-', end='')\n",
    "\n",
    "        # Store label and prediction\n",
    "        y_true.append(y_test.values[0])\n",
    "        y_pred.append(y_hat[0])\n",
    "        \n",
    "        # Store feature importance:\n",
    "        feat_import_per_fold.append(metaclf.feature_importances_)\n",
    "\n",
    "        #print(y_test.values, y_pred)\n",
    "        #print(f'F1 = {f1_score(y_test, y_pred, average=\"weighted\")}  \\taccuracy = {accuracy_score(y_test,y_pred)}')\n",
    "        summary_of_predictions_dict['Model'].append(model)\n",
    "        summary_of_predictions_dict['Dataset'].append(df.iloc[test_index,:].Dataset.values[0])\n",
    "        summary_of_predictions_dict['Oracle\\'s ST'].append(df.iloc[test_index,:].Best_ST.values[0])\n",
    "        summary_of_predictions_dict['Oracles\\'s Perf'].append(df.iloc[test_index,:].Max_F1_perf.values[0])\n",
    "        summary_of_predictions_dict['Meta-scaler\\'s ST'].append(y_hat[0])\n",
    "        summary_of_predictions_dict['Meta-scaler\\'s Perf'].append(df.iloc[test_index,:][y_hat[0]].values[0])\n",
    "    \n",
    "    feat_importances_dict[model] = feat_import_per_fold\n",
    "    summary_of_predictions = pd.DataFrame(summary_of_predictions_dict)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    \n",
    "    #print(f\"\\nAccuracy = {acc}\\t\\tF1 = {f1}\")\n",
    "    metamodel_performances_dict['Model'].append(model)\n",
    "    metamodel_performances_dict['Accuracy'].append(acc)\n",
    "    metamodel_performances_dict['F1'].append(f1)\n",
    "\n",
    "metamodel_performances = pd.DataFrame(metamodel_performances_dict)\n",
    "metamodel_performances.to_csv('../results/csv_tabs/metamodel_performances_loocv_RF.csv', encoding='utf8', index=False, float_format='%.4f')\n",
    "\n",
    "print(f'\\nTraining time: {training_time} seconds.')   \n",
    "print(f'Testing time: {testing_time} seconds.')       \n",
    "computing_times['RF'] = {'Testing': testing_time, 'Training': training_time, \n",
    "                            'Total': testing_time+training_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d61bb435-9997-492a-bf91-a43ce8e34041",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_ST_results = dataset.set_index(['Model', 'Dataset'])[['NS', 'SS', 'MMS', 'MAS', 'RS', 'QT']] \n",
    "meta_scaler_results = summary_of_predictions.set_index(['Model', 'Dataset'])\n",
    "all_results = pd.concat([static_ST_results, meta_scaler_results], axis = 1).reset_index()\n",
    "all_results.to_csv('../results/csv_tabs/summary_of_predictions_loocv_RF.csv', encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad579c4a-0921-46fc-847c-3321b13ce274",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = dataset.iloc[:, 2:-9].columns\n",
    "feat_import_means_per_model = {}\n",
    "models_names = dataset['Model'].unique()\n",
    "for model in models_names:\n",
    "    means = pd.DataFrame.from_records(feat_importances_dict[model], columns=feature_names).mean()\n",
    "    feat_import_means_per_model[model] = means\n",
    "feat_import_means_per_model = pd.DataFrame(feat_import_means_per_model).T\n",
    "feat_import_means_per_model.to_csv('../results/csv_tabs/feat_importances_RF.csv', encoding='utf8', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed10437-d2ce-4aaf-b5c1-eb1c3799bfba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Running the experiment with LOOCV. Meta-model: ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65e3dca3-b7f3-4001-940a-3f9558a8c8a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training metamodel for 'Model' Bagging.\n",
      "-----+--+-+-+----+--------+----+-----++++---+--+++---+--+++-+++--++-----+---+++-++-++-----++----+++++-+---++-----++-++--+---+--+-+++++--++++-+++-+-----+-+--+--+----------------++--++-----+---+++++++-+--+----+-+---++-++-+--+--+--+-+---+--+---+++-+--++++-+-+---+------+-+++--+-+---++------------------+\n",
      "Training metamodel for 'Model' GLVQ.\n",
      "-----+++++++---+-+---+----++++-++++++++++-+-++--+-+--+++++++++++-+++++++--++++--+---+-++--+++++++++++++++++++++++++++++++-+-++-+++++++++++++++++++--++-++++++++++++++-+-+-++--+++++++++-+++++++++++++++++++++++--++++--+++++++++++++++-+++++++++-++++++--++++++++++++++++++-++++++++-++++++++++-+++++++---++\n",
      "Training metamodel for 'Model' GP.\n",
      "-----+++++++-++++++++---+----+-+--++-+++++-++++--+---++++++-+++--++---------++-+--++-++-+-+++-+--++++--+++---+---+++++-+-+---+++--+++-+--++++-+--+-+-+-+++++-++++-+--+--------++++++++--+-+++-+++++++-++----++--++-----+---+++++-++++++---+---++-++-+++-+++-++++-++++-+--+++------+----++-----+++++----+--+-\n",
      "Training metamodel for 'Model' KNORAE.\n",
      "--++-++++++-+++++++-------++---+++-+++++-+++-+---+--+++-+----++--++----+--+----+-------++-+------++++---++---+-++-----++-+-+--+-+------+-++--------+---++--+++--+-+--------++---+------+--------+---+------+----+---+--+-----+-----+--------+----+---+---++--+-----++-+--------++------++++---+++++-+-++--++\n",
      "Training metamodel for 'Model' KNORAU.\n",
      "-----+++++++-++-+------+--------+----+++++-++--+---++---+++-+----++--+---+++-+-+-------+++++----++--+------+--+++++--++-----+---+-++++++-+++++++-+-+-+-+-++++---+-------+++-+---+++-++-----+--+++++++--+--------+-----+------+-----+--+-------+--+---+--+++--++----+-----+++++----+----++----------+-++-+---\n",
      "Training metamodel for 'Model' LCA.\n",
      "---+----++-------++--++--+-----+---------+++--+---------+----+-+++++----+-+--+-+--+-+---+--+-+----++---+-----+--++--------+-+--+--+--+-+-+---++----++---+------+------+--------+++------++----+-------+----------+-+--------------+-------+---+--++--+---++--+-+---+--------------+----++---------+---------\n",
      "Training metamodel for 'Model' MCB.\n",
      "+-+--+++++++-++-+++++-+--+-----++----+++++-+--++----++-+-++-++++-++--+-+-++----+-++++-+-+--++-++--+-++-+--+----+++-+-+--+--+-+----+--+--++-+-----++---+++-++---++----+-+------+---++---++---+--------------+----+----+++-----+------+--------+---+-+++--+++--+---+++----------+------+-++++----+-++------++-\n",
      "Training metamodel for 'Model' MLP.\n",
      "+++++++++++-+---++++++++++++-++++--+++++++-++-+-+------++-+---+--++++--+--+-++-++-----+++++++-+---++++--+++-+++++++-+++-++---++++-++-----+++--+--+-----++-+++++++-----------++--++--++---++---+++---+++++++++++--++-+------+-+-----+---+-+++++++-++--++--++++++----+--+--++--+---+++---++++---++++++++++++++\n",
      "Training metamodel for 'Model' OLA.\n",
      "--+--++-++++-++--++++-+--++++--++----+++++-++--+--+--++--++-+++---++----+-+--+-+----+----++-+++---+----+---+--+-++++-+++---------+------+-+----++++---++-+--+---+-+--+--+++-+-++++---+-------+-+----++----+---+-+-++-----++----+----+---+------+-+---++-++++-+----++++----++-----------++++---+++++------+++\n",
      "Training metamodel for 'Model' Percep.\n",
      "-------------++------------------++---++--+----+------+---+----++---++--------------------+-+-+-+-+---++------------+-+--++----------------+--+---------+---+--+----+------+-------------+--+-+---------------+---+--------+--+--++-----+--------+--++---++----+----+---+-+--+-------++++--+++-----+-++-----\n",
      "Training metamodel for 'Model' SVM_RBF.\n",
      "-----++--++++++++++++----------+++++-+++-+++++-+---++--++++++++++++--++++++-+++++---+-+-+-+++++++++++-++++-+--++++++++-----++-+---++++++++++---+--+-+++++++-+-+++--+--+-+---++--+++++++++--++--+++++++--+-++-+--+------+-++-+-++++++++-+++++++++-+---+---++++++-+++++++--++-------+---+++-----+++++++++---+-\n",
      "Training metamodel for 'Model' SVM_lin.\n",
      "-++++++++++++--++-++-+-+++---+-++----++++-+--+++++---+--+++-+++--++-------+-+----+--+---+-++----+++++--+++---+--++++++-+--++---+--++-----+++++-+-+-+-+-+++--+++++++++-++++--+---++--++--+-+++--+++++++++++++++---+-+----+++--+-++++-+--+++++++++++++-++--++--+-+-+-++++--++---+++---+-+++---+--+----------++\n",
      "Training time: 458.69456899771467 seconds.\n",
      "Testing time: 80.93847902258858 seconds.\n",
      "CPU times: user 4h 16min 50s, sys: 7min 42s, total: 4h 24min 32s\n",
      "Wall time: 11min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "training_time = 0\n",
    "testing_time = 0 \n",
    "# Creating DataFrames to store results:\n",
    "summary_of_predictions_dict = {'Model':[], 'Dataset':[], 'Oracle\\'s ST':[],\n",
    "                               'Oracles\\'s Perf':[], 'Meta-scaler\\'s ST':[], \n",
    "                               'Meta-scaler\\'s Perf':[]}\n",
    "\n",
    "metamodel_performances_dict = {'Model':[], 'Accuracy':[], 'F1': []}\n",
    "models_names = dataset['Model'].unique()\n",
    "feat_importances_dict = dict(zip(models_names, [[]]*len(models_names)))\n",
    "\n",
    "#print(f'Shape of original X: {dataset.iloc[:,2:-9].shape}.')\n",
    "for model in models_names: # Running for each of the 12 models\n",
    "    print(f'\\nTraining metamodel for \\'Model\\' {model}.')\n",
    "    df = dataset[dataset['Model'] == model]\n",
    "    df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "    # Separating X and y.\n",
    "    X = df.iloc[:, 2:-9] # Just the metafeatures.\n",
    "    y = df.iloc[:,-1] # Just the best ST.\n",
    "\n",
    "    # Splitting the dataset into the Training set and Test set folds, according to Leave One Out cross validation:\n",
    "    loo = LeaveOneOut()\n",
    "    feat_import_per_fold = []\n",
    "    y_true, y_pred = list(), list()\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        #print('.', end='')\n",
    "        # Separating training and test sets:\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # Filling missing values with a KNN imputer.  Each sample’s missing values are imputed\n",
    "        # using the mean value from n_neighbors nearest neighbors found in the training set. \n",
    "        imputer = KNNImputer(n_neighbors=2)\n",
    "        X_train = imputer.fit_transform(X_train)\n",
    "        X_test = imputer.transform(X_test)\n",
    "       \n",
    "        # Fitting the metamodel.\n",
    "        # Using class weights to mitigate the effects of class imbalance.\n",
    "        tic = time.perf_counter()\n",
    "        metaclf = ExtraTreesClassifier(class_weight='balanced', n_estimators=100, \n",
    "                                       n_jobs=-1, random_state=13)\n",
    "        metaclf.fit(X_train, y_train)\n",
    "        toc = time.perf_counter()\n",
    "        training_time += toc-tic\n",
    "        \n",
    "        # Evaluate model\n",
    "        tic = time.perf_counter()\n",
    "        y_hat = metaclf.predict(X_test)\n",
    "        toc = time.perf_counter()\n",
    "        testing_time += toc-tic\n",
    "\n",
    "        # Printing a '+' if instance was correctly classified, else prints '-':\n",
    "        if y_hat[0] == y_test.values[0]: print('+', end='')\n",
    "        else: print('-', end='')\n",
    "\n",
    "        # Store label and prediction\n",
    "        y_true.append(y_test.values[0])\n",
    "        y_pred.append(y_hat[0])\n",
    "        \n",
    "        # Store feature importance:\n",
    "        feat_import_per_fold.append(metaclf.feature_importances_)\n",
    "\n",
    "        summary_of_predictions_dict['Model'].append(model)\n",
    "        summary_of_predictions_dict['Dataset'].append(df.iloc[test_index,:].Dataset.values[0])\n",
    "        summary_of_predictions_dict['Oracle\\'s ST'].append(df.iloc[test_index,:].Best_ST.values[0])\n",
    "        summary_of_predictions_dict['Oracles\\'s Perf'].append(df.iloc[test_index,:].Max_F1_perf.values[0])\n",
    "        summary_of_predictions_dict['Meta-scaler\\'s ST'].append(y_hat[0])\n",
    "        summary_of_predictions_dict['Meta-scaler\\'s Perf'].append(df.iloc[test_index,:][y_hat[0]].values[0])\n",
    "    \n",
    "    feat_importances_dict[model] = feat_import_per_fold\n",
    "    summary_of_predictions = pd.DataFrame(summary_of_predictions_dict)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    \n",
    "    #print(f\"\\nAccuracy = {acc}\\t\\tF1 = {f1}\")\n",
    "    metamodel_performances_dict['Model'].append(model)\n",
    "    metamodel_performances_dict['Accuracy'].append(acc)\n",
    "    metamodel_performances_dict['F1'].append(f1)\n",
    "\n",
    "metamodel_performances = pd.DataFrame(metamodel_performances_dict)\n",
    "metamodel_performances.to_csv('../results/csv_tabs/metamodel_performances_loocv_ET.csv', encoding='utf8', index=False, float_format='%.4f')\n",
    "\n",
    "print(f'\\nTraining time: {training_time} seconds.')   \n",
    "print(f'Testing time: {testing_time} seconds.')       \n",
    "computing_times['ET'] = {'Testing': testing_time, 'Training': training_time, \n",
    "                            'Total': testing_time+training_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1ddd9ea-8631-4d1b-a11d-3f4bf546f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_ST_results = dataset.set_index(['Model', 'Dataset'])[['NS', 'SS', 'MMS', 'MAS', 'RS', 'QT']] \n",
    "meta_scaler_results = summary_of_predictions.set_index(['Model', 'Dataset'])\n",
    "all_results = pd.concat([static_ST_results, meta_scaler_results], axis = 1).reset_index()\n",
    "all_results.to_csv('../results/csv_tabs/summary_of_predictions_loocv_ET.csv', encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "019f6c47-4a9b-499b-9706-f8ae35cc5825",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = dataset.iloc[:, 2:-9].columns\n",
    "feat_import_means_per_model = {}\n",
    "models_names = dataset['Model'].unique()\n",
    "for model in models_names:\n",
    "    means = pd.DataFrame.from_records(feat_importances_dict[model], columns=feature_names).mean()\n",
    "    feat_import_means_per_model[model] = means\n",
    "feat_import_means_per_model = pd.DataFrame(feat_import_means_per_model).T\n",
    "feat_import_means_per_model.to_csv('../results/csv_tabs/feat_importances_ET.csv', encoding='utf8', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958be506-bf08-40de-a98a-149e9841169f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Running the experiment with LOOCV. Meta-model: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "397440c7-62e4-4c05-93ae-2c689e715ab4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training metamodel for 'Model' Bagging.\n",
      "----++--+-+---------+-----+--+-+-----++++---+--++-------+++-+++--++--+--+++-++--+---+----+++----+++++-+---++---+-++-++--+++----+-++++++-++++-+++-+-----+-+--+--+------------++--++--++-+--++---++-++++-+-+++-++--+--++-----+--+-----+------------++--+---+++-+-----+--------++---+-+---++------------------+\n",
      "Training metamodel for 'Model' GLVQ.\n",
      "-++--++++++++--+++---+-+--+-++-++++++++++-+-++--+-+--+-+++++++++-+++++++-+--+--++-----++--+++++++++++-++++++++-++++-+++++---++-++-++++++++++++++++--++-++++++++++++++-+-+-+++-+++++++++-+++++++++++++++++++++++-+++++--+++++++++++++++-+++++++++-+++++---++++++++++++++++++-++++++++-++++++++++++++++++---++\n",
      "Training metamodel for 'Model' GP.\n",
      "-----++++++--++++++-+---+----+-+-+-+-+++++-+++-----+--+++++-+++--++----+-+--++-+--++-++++-+++-+--++++--+++---+--++++++-+--+-++++--+++-+--+++----++---+-+++++-++++-+------------++++++++-+-+++-+++++++-++----+---+++----+---+++++++++-++-+++---++-++-+++--++-++++-++++-++-+++--+---++----+-----+-+++-+-----+-\n",
      "Training metamodel for 'Model' KNORAE.\n",
      "--++-++++++-++++++++-----+++---+++-+++++-+++-+---+---++-+---+++--++-+--+--+----+-+-----++----+---+-++-+-++-----++-----+-----+-+-+------+-----------+---++--+++--+----------++------+---++-+-----+---+--+---+----+-+-+--+--+-+--+--++---------+---+---+--+++--------------+-----++------++++---+++++-+-++--+-\n",
      "Training metamodel for 'Model' KNORAU.\n",
      "-----++++++++++-+------++-----+-+----++++++++--+-+------+++-+----++-------+--+-+-------++-++----++--+------+--+-++++++------+-----+++++--+++-+-+-+---+---+--+---+-------++------+++-++-----+--+++++++--+--+-----+---+--+-----+-----+--+---+---+--+--++---++--+-----+++---+-+++----+----++----+--+--+-+------\n",
      "Training metamodel for 'Model' LCA.\n",
      "--------++------+++---+--+-----------+---+++--+---------+----+-+-+---+----+---++--+--+-------+-+--++--++-----+--++----++--+-------+--+-+-+---++----++--+-------+----------------+--+-+-++-----+-----------------+-+--------------++-------+----+-+---+---++--+-----+--+--+++-+-----+---++------+--+---------\n",
      "Training metamodel for 'Model' MCB.\n",
      "+-+--++++++++++-+++++-+--+-----++----+++++-+-+--+----+-+-++-++++-++-++-+-++---++-++++-+-+--++-++--+--+----++---+++-+-++-+-+-------++-+--++-++---+++---+++--+---++----+++------+---+----++------+-+----++--------+--+-+++----------+-+-+------+---+--++---+++++----+++---------+--+---+-+++++---+-++------++-\n",
      "Training metamodel for 'Model' MLP.\n",
      "+++++++++++-+---++++++++++++-++++--+++++++-++-+--+-----++-+---+--++----++-+----+-+----+-+++++-----++++--+++--++++++++++--+---++++-++-----++++----------++++++++++-+---------++-+++--++-+-++---+++---+++++++++++--++-----+--+-+----++---+-+++++++-++--++--++++++----++-+--++--+---+++---++++---++++++-+++-+++\n",
      "Training metamodel for 'Model' OLA.\n",
      "-----++--++++++--++++-++-++-+--++--+-+++++-++--+--++-++--++-+++---+----+--+--+-+----------+---+---+-------+---++++++-++----+----------------++--+++-+-+--+-++--++-++----+++-+-+++-+--+----+--+------++-+--+---+-+-++-----++----+----+-+-+-+------+---++--++-+---+-++++-----+-----------++++---+++++------++-\n",
      "Training metamodel for 'Model' Percep.\n",
      "-------------+++-----+-----------++--+++--+------------+--+----+----+----+---+-----------++---+-++-----+------------+-+-+++-------++----------+--+------+------+----+---+-------+-----------+---------------+---+-+-------++--+--++-----+--------+--++---+++-------------------+-----++++--++++----+-++-+--+\n",
      "Training metamodel for 'Model' SVM_RBF.\n",
      "-----++--++++++++++-+----------++-++-+++-+++++-++--+---++++++++++++--++++++-+-+++--++-+---+++++++++++-++++++---+++++++----++---+-+++++++++++---+--+++++++++-+-+++-+------+---++-++++++++-++++--+++++++--++++-+--+-----++-++-+-+++++-++-+++++++++-+--++---+++-++-+++++++--++-------++--+++----++++++++++--++-\n",
      "Training metamodel for 'Model' SVM_lin.\n",
      "-++++++++++++--++-++-+-+++--+++++----++++-+--+++++------+++-+++-+++--++-+++-++---+--+---+-++-+--+++++--+++---+--+++-++-+--+++--+--++-----+++++---+---+-+++--+++++++++-++++--+---++--++------+--+++++++++++++++---+-++---+-+--+-++-+-+--++++++++++++--+++-++--+-+---++++--+++---++---+-+++---+--++--------++-\n",
      "Training time: 4333.636337836506 seconds.\n",
      "Testing time: 1.2999204655643553 seconds.\n",
      "CPU times: user 1h 12min 58s, sys: 656 ms, total: 1h 12min 58s\n",
      "Wall time: 1h 12min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#%%capture XGB_output\n",
    "\n",
    "training_time = 0  \n",
    "testing_time = 0    \n",
    "# Creating DataFrames to store results:\n",
    "summary_of_predictions_dict = {'Model':[], 'Dataset':[], 'Oracle\\'s ST':[],\n",
    "                               'Oracles\\'s Perf':[], 'Meta-scaler\\'s ST':[], \n",
    "                               'Meta-scaler\\'s Perf':[]}\n",
    "\n",
    "metamodel_performances_dict = {'Model':[], 'Accuracy':[], 'F1': []}\n",
    "\n",
    "models_names = dataset['Model'].unique()\n",
    "\n",
    "#print(f'Shape of original X: {dataset.iloc[:,2:-9].shape}.')\n",
    "for model in models_names: # Running for each of the 12 models\n",
    "    print(f'\\nTraining metamodel for \\'Model\\' {model}.')\n",
    "    df = dataset[dataset['Model'] == model]\n",
    "    df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "    # Separating X and y.\n",
    "    X = df.iloc[:, 2:-9] # Just the metafeatures.\n",
    "    y = df.iloc[:,-1] # Just the best ST.\n",
    "    \n",
    "\n",
    "    # Splitting the dataset into the Training set and Test set folds, according to Leave One Out cross validation:\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    y_true, y_pred = list(), list()\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        #print('.', end='')\n",
    "        # Separating training and test sets:\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # Filling missing values with a KNN imputer.  Each sample’s missing values are imputed\n",
    "        # using the mean value from n_neighbors nearest neighbors found in the training set. \n",
    "        imputer = KNNImputer(n_neighbors=2)\n",
    "        X_train = imputer.fit_transform(X_train)\n",
    "        X_test = imputer.transform(X_test)\n",
    "        \n",
    "        # Fitting the metamodel.\n",
    "        # Using class weights to mitigate the effects of class imbalance.\n",
    "        tic = time.perf_counter() \n",
    "        metaclf = xgb.XGBClassifier(class_weight='balanced', n_jobs=-1, \n",
    "                                    random_state=13)\n",
    "        metaclf.fit(X_train, y_train)\n",
    "        toc = time.perf_counter() \n",
    "        training_time += toc-tic\n",
    "        \n",
    "        # Evaluate model\n",
    "        tic = time.perf_counter() \n",
    "        y_hat = metaclf.predict(X_test)\n",
    "        toc = time.perf_counter() \n",
    "        testing_time += toc-tic\n",
    "        \n",
    "        # Printing a '+' if instance was correctly classified, else prints '-':\n",
    "        if y_hat[0] == y_test.values[0]: print('+', end='')\n",
    "        else: print('-', end='')\n",
    "\n",
    "        # store\n",
    "        y_true.append(y_test.values[0])\n",
    "        y_pred.append(y_hat[0])\n",
    "\n",
    "        summary_of_predictions_dict['Model'].append(model)\n",
    "        summary_of_predictions_dict['Dataset'].append(df.iloc[test_index,:].Dataset.values[0])\n",
    "        summary_of_predictions_dict['Oracle\\'s ST'].append(df.iloc[test_index,:].Best_ST.values[0])\n",
    "        summary_of_predictions_dict['Oracles\\'s Perf'].append(df.iloc[test_index,:].Max_F1_perf.values[0])\n",
    "        summary_of_predictions_dict['Meta-scaler\\'s ST'].append(y_hat[0])\n",
    "        summary_of_predictions_dict['Meta-scaler\\'s Perf'].append(df.iloc[test_index,:][y_hat[0]].values[0])\n",
    "    summary_of_predictions = pd.DataFrame(summary_of_predictions_dict)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    \n",
    "    #print(f\"\\nAccuracy = {acc}\\t\\tF1 = {f1}\")\n",
    "    metamodel_performances_dict['Model'].append(model)\n",
    "    metamodel_performances_dict['Accuracy'].append(acc)\n",
    "    metamodel_performances_dict['F1'].append(f1)\n",
    "\n",
    "metamodel_performances = pd.DataFrame(metamodel_performances_dict)\n",
    "metamodel_performances.to_csv('../results/csv_tabs/metamodel_performances_loocv_XBG.csv', encoding='utf8', index=False, float_format='%.4f')\n",
    "print(f'\\nTraining time: {training_time} seconds.')   \n",
    "print(f'Testing time: {testing_time} seconds.')       \n",
    "computing_times['XGB'] = {'Testing': testing_time, 'Training': training_time, \n",
    "                            'Total': testing_time+training_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37fefcff-0be3-4dad-93dc-9bdb3a0a0d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_ST_results = dataset.set_index(['Model', 'Dataset'])[['NS', 'SS', 'MMS', 'MAS', 'RS', 'QT']] \n",
    "meta_scaler_results = summary_of_predictions.set_index(['Model', 'Dataset'])\n",
    "all_results = pd.concat([static_ST_results, meta_scaler_results], axis = 1).reset_index()\n",
    "all_results.to_csv('../results/csv_tabs/summary_of_predictions_loocv_XGB.csv', encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3094f2-8e9e-4938-914b-8a3c4798d256",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Running the experiment with LOOCV. Meta-model: Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "408df7ca-6e09-4687-a7a8-610aad70b41b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training metamodel for 'Model' Bagging.\n",
      "--+-++--+-+------------+--+----+--++-++++------++----++-+++-+++--++-----+----+------+----+++-----+++++----+------++-++-+--+-------+++---++++-++-+-+-+--+----++-+--------+---+---++--++-+---+---++-+-++-+--++-+-+------+---------++--------+-++---+++-++-+++--+-+---+----+++---+--+-----++-------+-----+----+\n",
      "Training metamodel for 'Model' GLVQ.\n",
      "--+--++++++++------+-+----++---++-+--++++---+-+-+-+----+++++++++-+++---+--+-++-+----+++---+++++++++++++++++--+++++++++-++---+--+-+++++++++++---++-++-+-+++++++++++++----------+-+++++++--++-+++++++++++++++++++--+++++-+++++++++++++++-+++++++++-+++++---++++++++++++++--+++--+++++++++++++++++++--++++----+\n",
      "Training metamodel for 'Model' GP.\n",
      "----+++++++-+-+----++----------+--+--+++++++----+--+---++++++++++++-+--+---++--+---+-++---++---+-++++--++++---+-+++++++-----+-----++--++++++--+-+-+--+-+++-+--++---+--+----+---+++-+++------+-+++++++--+----++-+--+-----+--+++-+--+--+---++--+---+--+++-++++++-----+--+--++-------++-+-++---------+++-------\n",
      "Training metamodel for 'Model' KNORAE.\n",
      "-----++++++-++++-+++-+-+--+----+++---+++-+++-+---------------+---+---+----+----+----------+------++-+---++-+----+----+---+-------------+--------+-+----+--++----+----------------+-----++--------+---------+-+--+---++-+++--+--------------------+--+++--++--+++---------------++------+++--+-+-+++-+-+-----\n",
      "Training metamodel for 'Model' KNORAU.\n",
      "--+-+++++++++++----+------------+-+--++++-+-------------+++-+--+-++------+-----++-------+-++----++--++-----+--+--++--+----+----++-++++-+-+++-++--+--++-+----+--+--------------+-+++-++-----+---++-+++--+---++----+-------+---+---+--------+-++---+---+---++--+-----+-++-++-------------++---------++-++--++-\n",
      "Training metamodel for 'Model' LCA.\n",
      "---+--+------+---++---+--+-----+-----+---+++------+-----+-+-+---+++--+--+----+-++----+-----+++----++---+--------+---------+-+---+-+--+---+---++-----+----------+------+---------++---+------------+----++---------------+--+-+--+-++------+---++-+---+--+++--+-----+--++-+---+----++---++---------+-----+---\n",
      "Training metamodel for 'Model' MCB.\n",
      "--+--++++++--++-+++++-+-+++-+--++----++++------+---+-+---++-+++--++--++++++----++++-+------+--+---++-+----+----+++-+-++------+-+--++-++++-----+-+-++--++++-+---++-----++---++---+-+-+--+------+-----+----+------+-+---+---++------+-+---+-+-++---+--++-+-+++-+----+-----+----+-------+-+++++------+------++-\n",
      "Training metamodel for 'Model' MLP.\n",
      "+++++++++++-+--+++++++++++++-++-+-----++------+--------++-+-+-+-+++-++-+-+------+-++--+--++++-+---++----+++----+-++-++--++--++++--+++-+-++++++-++------+++-++-+++---+-------++--++--++++-------+++--+++-+++++++--+--+---+--+++++-------+-+++++++-++--+--++++++-+---+--+--++------+++---+++++--+-++++-+-+++++\n",
      "Training metamodel for 'Model' OLA.\n",
      "-----++-+++---+--++-+-++-+----+++----++++-+--------+-----++-++++--+-------+-+--+-+--------+---+---++------++---+-+---++--+-+-------+--+-+---+---+-+---+--------++--+--------------+-++--++---+--+-+-++----+------+----+--+---+-+---+-------------+---++-++++-+--+-+--------------------++++--++---+---+--+--\n",
      "Training metamodel for 'Model' Percep.\n",
      "----+---++---++--------------+--------------------------+---+--++-----------++----+-------+-+---+-+---++---------+--++---+-+----+-+----+---+--+----------+-----+----+---+--------+-+-+++--+-+-+-+--+----+---+----------+--++--+--++-----++--+----+---+---+++---+----+--+--------------++++-+-+-+---+-++-----\n",
      "Training metamodel for 'Model' SVM_RBF.\n",
      "--+--+---++++++++++---+-----+--++++--++---+--++++--+---++++++++-+++---++-+--+------+++++--++++---++++--+++-+--++-++-++-----------++++---++++----++---+++++--+-+++------++--+----++--+++-----+--++++++++-+-++-+--+--+---+-++-+-+++++-++-+++++++++-+---++--++++++-++-++++--++-----------+++-----++++++-++--+++\n",
      "Training metamodel for 'Model' SVM_lin.\n",
      "-----+++++++---+---+++-----+---++----++++-----+++----+--+++-+++-+++---+---+-++-+-+-+------+++----++++---++--+--+++++++-+---+--++--++-----+++--+---+-++-+++--+-++-+++---+++-+--++++--+++-----+--++++++++++++++++--+++----++-----+-+-----+-++++++++++--+-+-++-++---+-++++-+++--+-++---+-+++---+-----------+---\n",
      "Training time: 145.58184926304966 seconds.\n",
      "Testing time: 128.34692481835373 seconds.\n",
      "CPU times: user 4min 45s, sys: 31.5 s, total: 5min 17s\n",
      "Wall time: 5min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#%%capture BAG_output\n",
    "\n",
    "training_time = 0  \n",
    "testing_time = 0   \n",
    "# Creating DataFrames to store results:\n",
    "summary_of_predictions_dict = {'Model':[], 'Dataset':[], 'Oracle\\'s ST':[],\n",
    "                               'Oracles\\'s Perf':[], 'Meta-scaler\\'s ST':[], \n",
    "                               'Meta-scaler\\'s Perf':[]}\n",
    "\n",
    "metamodel_performances_dict = {'Model':[], 'Accuracy':[], 'F1': []}\n",
    "\n",
    "models_names = dataset['Model'].unique()\n",
    "\n",
    "#print(f'Shape of original X: {dataset.iloc[:,2:-9].shape}.')\n",
    "for model in models_names: # Running for each of the 12 models\n",
    "    print(f'\\nTraining metamodel for \\'Model\\' {model}.')\n",
    "    df = dataset[dataset['Model'] == model]\n",
    "    df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "    # Separating X and y.\n",
    "    X = df.iloc[:, 2:-9] # Just the metafeatures.\n",
    "    y = df.iloc[:,-1] # Just the best ST.\n",
    "    \n",
    "\n",
    "    # Splitting the dataset into the Training set and Test set folds, according to Leave One Out cross validation:\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    y_true, y_pred = list(), list()\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        #print('.', end='')\n",
    "        # Separating training and test sets:\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # Filling missing values with a KNN imputer.  Each sample’s missing values are imputed\n",
    "        # using the mean value from n_neighbors nearest neighbors found in the training set. \n",
    "        imputer = KNNImputer(n_neighbors=2)\n",
    "        X_train = imputer.fit_transform(X_train)\n",
    "        X_test = imputer.transform(X_test)\n",
    "\n",
    "        # Fitting the metamodel.\n",
    "        # Using class weights to mitigate the effects of class imbalance.\n",
    "        tic = time.perf_counter()\n",
    "        metaclf = BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight='balanced'),\n",
    "                                    n_jobs=-1, max_samples=0.5, max_features=1, n_estimators=100,\n",
    "                                    bootstrap_features=False, random_state=13)\n",
    "        metaclf.fit(X_train, y_train)\n",
    "        toc = time.perf_counter()  \n",
    "        training_time += toc-tic   \n",
    "        \n",
    "        # Evaluate model\n",
    "        tic = time.perf_counter()        \n",
    "        y_hat = metaclf.predict(X_test)\n",
    "        toc = time.perf_counter()  \n",
    "        testing_time += toc-tic   \n",
    "        \n",
    "        # Printing a '+' if instance was correctly classified, else prints '-':\n",
    "        if y_hat[0] == y_test.values[0]: print('+', end='')\n",
    "        else: print('-', end='')\n",
    "\n",
    "        # store\n",
    "        y_true.append(y_test.values[0])\n",
    "        y_pred.append(y_hat[0])\n",
    "\n",
    "        summary_of_predictions_dict['Model'].append(model)\n",
    "        summary_of_predictions_dict['Dataset'].append(df.iloc[test_index,:].Dataset.values[0])\n",
    "        summary_of_predictions_dict['Oracle\\'s ST'].append(df.iloc[test_index,:].Best_ST.values[0])\n",
    "        summary_of_predictions_dict['Oracles\\'s Perf'].append(df.iloc[test_index,:].Max_F1_perf.values[0])\n",
    "        summary_of_predictions_dict['Meta-scaler\\'s ST'].append(y_hat[0])\n",
    "        summary_of_predictions_dict['Meta-scaler\\'s Perf'].append(df.iloc[test_index,:][y_hat[0]].values[0])\n",
    "    summary_of_predictions = pd.DataFrame(summary_of_predictions_dict)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    \n",
    "    #print(f\"\\nAccuracy = {acc}\\t\\tF1 = {f1}\")\n",
    "    metamodel_performances_dict['Model'].append(model)\n",
    "    metamodel_performances_dict['Accuracy'].append(acc)\n",
    "    metamodel_performances_dict['F1'].append(f1)\n",
    "\n",
    "metamodel_performances = pd.DataFrame(metamodel_performances_dict)\n",
    "metamodel_performances.to_csv('../results/csv_tabs/metamodel_performances_loocv_BAG.csv', encoding='utf8', index=False, float_format='%.4f')\n",
    "\n",
    "print(f'\\nTraining time: {training_time} seconds.')   \n",
    "print(f'Testing time: {testing_time} seconds.')       \n",
    "computing_times['BAG'] = {'Testing': testing_time, 'Training': training_time, \n",
    "                            'Total': testing_time+training_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "650758a6-a7b9-474f-b2c7-d941275bb624",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_ST_results = dataset.set_index(['Model', 'Dataset'])[['NS', 'SS', 'MMS', 'MAS', 'RS', 'QT']] \n",
    "meta_scaler_results = summary_of_predictions.set_index(['Model', 'Dataset'])\n",
    "all_results = pd.concat([static_ST_results, meta_scaler_results], axis = 1).reset_index()\n",
    "all_results.to_csv('../results/csv_tabs/summary_of_predictions_loocv_BAG.csv', encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff04a87-c14f-4deb-96d5-7faa6da01571",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Running the experiment with LOOCV. Meta-model: KNORAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86770cb5-1815-4cf5-8a44-a4d17610b952",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training metamodel for 'Model' Bagging.\n",
      "-----+--+-+-+----+-------------+-+---+---------++---+++-+++-++++-++--+--+---++----+-+-----++-+--+++++-+----------+--++---+--------+++-+-++++-+++-+-----+-+++++-++--+----+--+----++--++-+---+--++-+++-+-+-++--+---+------++---+-----+-+------+----++----+--+----+---------+---+---------+++--------------+---\n",
      "Training metamodel for 'Model' GLVQ.\n",
      "-+---+++++++-------+-+-+----+--++++++++++---+---+-+--+-+++++++++-++++-++-++-++-+----++++--+++++++++++-++++++++-++++-+++++-+-+--++-++++++++++++++++++++-++++++++++++++-+-++-+--+++++++++-++++++++++++++++++++++++++++---+++++++++++++++++++++++++-++++++--+++++++++++++++-++-++++++++-++++++++++-++++++++--+-\n",
      "Training metamodel for 'Model' GP.\n",
      "+----++++++--+++++++--+-+------+-+---++++---++-++-----+-+++-+++--++--+---+-++--+---++++-+-+++-+--++++-++++-+----++++++-+-----+-+--+++-+--++++----+---+-+++++-++++----------+----++++++--+-+++-+++++++--++-------+------+-+-+++-+++---+--+++---+---------+-+-++++--++--+--+++--+----+-+-++-+---+++++-+----++-\n",
      "Training metamodel for 'Model' KNORAE.\n",
      "--+---+++++-+++++++-------+----++--+++++-+++-+-+-+---+--+----+---++--+-+--++---+-+------++---+---+++----+++-++-++-+--------+--+-+------+---+-----------+-----+----+---------+----------------+----+-------------+------+----++-----+----+++-+------------++-------+------------++------+------++-++---+-+-+-\n",
      "Training metamodel for 'Model' KNORAU.\n",
      "-----+--+++-++--++--+-++----+---+----++++-+++--+-+------+++-+----++--+---+--++------+--+-++---+-++--+------+--+-+++--++---+------+++++-+-+++-+++-+++-+---+-----++-+----+--------+++-++--------++--++---+---++-+++-----+------+-+-+-----+---------+-------++--+-+----+----+++++-------+-+---------+-+-+-+----\n",
      "Training metamodel for 'Model' LCA.\n",
      "-----------+--+++-+----+---+-------------++++----++----++-+--+-++-----+---+---+++---++-----+-+-+------++-----+--+--+----+++----+--+-++++-+---++---+-+---+--+------+---+-------------+---+-++----+------------+--+---+------+------+---------+--+---------+++----+-++--++-+---+---------+----------+------+--\n",
      "Training metamodel for 'Model' MCB.\n",
      "-++--++++++-+++-++-++-+--+-----++----+++++-----------+--+++-+++--++----++++---+--+-+--+-----+-++--+--+-+--+-+--+++-+------+--+---++------+-+--+---+---+----+----+---++++-----++--++-+--+------+++-+----+-----------+--++----------+--------------+--+----++++-----+-+------------++--+--+++----+-++------++-\n",
      "Training metamodel for 'Model' MLP.\n",
      "-++++++++++---++-+++++++-+++--+++--+++++-+-+-----------+--+---++-++++--+--+----++-----++-++++-----++++-++++--+++-++-++-------++++-++--+--+++--+-+-+----++--++++++-----------++--++-+++---++---+++-+-+++++-+++++--+--+-+-+-++++----++---+-+++++++-+---++---++++-+---+--+--++-+----+++---++++---+++++--+++++++\n",
      "Training metamodel for 'Model' OLA.\n",
      "-----++--++++++---+++-+--+-+----+--+-+++++-+-------------++-++----+----++------+----+-----+++++-+-+----+---+--+++-++--------------+----+--++---++-+-+-------+-----+------+-+-----------++--+-+-+-----+----+------+++-+---++--+-----+----+--+---+-+---+++--+----++---+------+---------+-+-++----++++----+-++-\n",
      "Training metamodel for 'Model' Percep.\n",
      "--+----+--+--++--+-----------+-+-+---+-----------++----++--------------+-----------+------+-------+---++------------------++++----+------+--------------+---+--+-----------+---------------------+-----+---------++----+-+-+--+---+-----+---++---+--++---+++--------+---++---+--------+++--++++----+-++++---\n",
      "Training metamodel for 'Model' SVM_RBF.\n",
      "-----++--++++++++++-+--+-+-----+++++-+++-+++++-++--+---++++++++++++--++-+++-+-++------+-++++++-++++++-++++-+-+-++++-++---+--+-+--+++++++++++---+--++-+++++--+-++------+-++---++-+++++++++-+++--+++++++--+-++-+--+------+-++-+-++-++++--+++++++++-+-------+++-++-+++++++--+++----------+++-----++++++-++---+-\n",
      "Training metamodel for 'Model' SVM_lin.\n",
      "--++++++++++++-+--+--+-+++---+-++----++++-+--+++-+-+-+--+++-+++--+++-++--+-+++---+--+---+-++----+++++--+++---+--+++-++-+---+---+--++-+---+++-----+-----+++-++-+++++++-+-+---+---++--+++----+++++++++++++++++++---++-------+--+-+--+-+--++++++++++-+-++-+--+-++-+---+--+--+++---++---+-+++------++-----------\n",
      "Training time: 276.1003168527968 seconds.\n",
      "Testing time: 28.68369330931455 seconds.\n",
      "CPU times: user 4min 25s, sys: 21.3 s, total: 4min 47s\n",
      "Wall time: 5min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#%%capture KNORAE_output\n",
    "\n",
    "\n",
    "training_time = 0  \n",
    "testing_time = 0 \n",
    "# Creating DataFrames to store results:\n",
    "summary_of_predictions_dict = {'Model':[], 'Dataset':[], 'Oracle\\'s ST':[],\n",
    "                               'Oracles\\'s Perf':[], 'Meta-scaler\\'s ST':[], \n",
    "                               'Meta-scaler\\'s Perf':[]}\n",
    "\n",
    "metamodel_performances_dict = {'Model':[], 'Accuracy':[], 'F1': []}\n",
    "\n",
    "models_names = dataset['Model'].unique()\n",
    "\n",
    "#print(f'Shape of original X: {dataset.iloc[:,2:-9].shape}.')\n",
    "for model in models_names: # Running for each of the 12 models\n",
    "    print(f'\\nTraining metamodel for \\'Model\\' {model}.')\n",
    "    df = dataset[dataset['Model'] == model]\n",
    "    df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "    # Separating X and y.\n",
    "    X = df.iloc[:, 2:-9] # Just the metafeatures.\n",
    "    y = df.iloc[:,-1] # Just the best ST.\n",
    "    \n",
    "\n",
    "    # Splitting the dataset into the Training set and Test set folds, according to Leave One Out cross validation:\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    y_true, y_pred = list(), list()\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        #print('.', end='')\n",
    "        # Separating training and test sets:\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # Filling missing values with a KNN imputer.  Each sample’s missing values are imputed\n",
    "        # using the mean value from n_neighbors nearest neighbors found in the training set. \n",
    "        imputer = KNNImputer(n_neighbors=2)\n",
    "        X_train = imputer.fit_transform(X_train)\n",
    "        X_test = imputer.transform(X_test)\n",
    "\n",
    "        # Fitting the metamodel.\n",
    "        # Using class weights to mitigate the effects of class imbalance.\n",
    "        pool = BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight='balanced'),\n",
    "                                 n_jobs=-1, max_samples=0.5, n_estimators=100,\n",
    "                                 bootstrap_features=False, max_features=1.0, random_state=13)\n",
    "        tic = time.perf_counter()\n",
    "        pool.fit(X_train, y_train)\n",
    "        metaclf = KNORAE(pool_classifiers = pool, n_jobs=-1)\n",
    "        metaclf.fit(X_train, y_train)\n",
    "        toc = time.perf_counter()\n",
    "        training_time += toc-tic\n",
    "        \n",
    "        # Evaluate model\n",
    "        tic = time.perf_counter()\n",
    "        y_hat = metaclf.predict(X_test)\n",
    "        toc = time.perf_counter()\n",
    "        testing_time += toc-tic\n",
    "        \n",
    "        # Printing a '+' if instance was correctly classified, else prints '-':\n",
    "        if y_hat[0] == y_test.values[0]: print('+', end='')\n",
    "        else: print('-', end='')\n",
    "\n",
    "        # store\n",
    "        y_true.append(y_test.values[0])\n",
    "        y_pred.append(y_hat[0])\n",
    "\n",
    "        summary_of_predictions_dict['Model'].append(model)\n",
    "        summary_of_predictions_dict['Dataset'].append(df.iloc[test_index,:].Dataset.values[0])\n",
    "        summary_of_predictions_dict['Oracle\\'s ST'].append(df.iloc[test_index,:].Best_ST.values[0])\n",
    "        summary_of_predictions_dict['Oracles\\'s Perf'].append(df.iloc[test_index,:].Max_F1_perf.values[0])\n",
    "        summary_of_predictions_dict['Meta-scaler\\'s ST'].append(y_hat[0])\n",
    "        summary_of_predictions_dict['Meta-scaler\\'s Perf'].append(df.iloc[test_index,:][y_hat[0]].values[0])\n",
    "    summary_of_predictions = pd.DataFrame(summary_of_predictions_dict)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    \n",
    "    #print(f\"\\nAccuracy = {acc}\\t\\tF1 = {f1}\")\n",
    "    metamodel_performances_dict['Model'].append(model)\n",
    "    metamodel_performances_dict['Accuracy'].append(acc)\n",
    "    metamodel_performances_dict['F1'].append(f1)\n",
    "\n",
    "metamodel_performances = pd.DataFrame(metamodel_performances_dict)\n",
    "metamodel_performances.to_csv('../results/csv_tabs/metamodel_performances_loocv_KNORAE.csv', encoding='utf8', index=False, float_format='%.4f')\n",
    "\n",
    "print(f'\\nTraining time: {training_time} seconds.')   \n",
    "print(f'Testing time: {testing_time} seconds.')       \n",
    "computing_times['KNORAE'] = {'Testing': testing_time, 'Training': training_time, \n",
    "                            'Total': testing_time+training_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eeca8e39-26b5-4508-af1a-82a8f5477c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_ST_results = dataset.set_index(['Model', 'Dataset'])[['NS', 'SS', 'MMS', 'MAS', 'RS', 'QT']] \n",
    "meta_scaler_results = summary_of_predictions.set_index(['Model', 'Dataset'])\n",
    "all_results = pd.concat([static_ST_results, meta_scaler_results], axis = 1).reset_index()\n",
    "all_results.to_csv('../results/csv_tabs/summary_of_predictions_loocv_KNORAE.csv', encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfbd465-f108-447b-a1a5-71caa1530c47",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Running the experiment with LOOCV. Meta-model: KNORAU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f6f9262-ee15-494c-b3c8-1598c6b560f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture KNORAU_output\n",
    "%%time\n",
    "\n",
    "training_time = 0  \n",
    "testing_time = 0    \n",
    "# Creating DataFrames to store results:\n",
    "summary_of_predictions_dict = {'Model':[], 'Dataset':[], 'Oracle\\'s ST':[],\n",
    "                               'Oracles\\'s Perf':[], 'Meta-scaler\\'s ST':[], \n",
    "                               'Meta-scaler\\'s Perf':[]}\n",
    "\n",
    "metamodel_performances_dict = {'Model':[], 'Accuracy':[], 'F1': []}\n",
    "\n",
    "models_names = dataset['Model'].unique()\n",
    "\n",
    "#print(f'Shape of original X: {dataset.iloc[:,2:-9].shape}.')\n",
    "for model in models_names: # Running for each of the 12 models\n",
    "    print(f'\\nTraining metamodel for \\'Model\\' {model}.')\n",
    "    df = dataset[dataset['Model'] == model]\n",
    "    df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "    # Separating X and y.\n",
    "    X = df.iloc[:, 2:-9] # Just the metafeatures.\n",
    "    y = df.iloc[:,-1] # Just the best ST.\n",
    "    \n",
    "    # Splitting the dataset into the Training set and Test set folds, according to Leave One Out cross validation:\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    y_true, y_pred = list(), list()\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        #print('.', end='')\n",
    "        # Separating training and test sets:\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # Filling missing values with a KNN imputer.  Each sample’s missing values are imputed\n",
    "        # using the mean value from n_neighbors nearest neighbors found in the training set. \n",
    "        imputer = KNNImputer(n_neighbors=2)\n",
    "        X_train = imputer.fit_transform(X_train)\n",
    "        X_test = imputer.transform(X_test)\n",
    "\n",
    "        # Fitting the metamodel.\n",
    "        # Using class weights to mitigate the effects of class imbalance.\n",
    "        pool = BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight='balanced'),\n",
    "                                 n_jobs=-1, max_samples=0.5, n_estimators=100,\n",
    "                                 bootstrap_features=False, max_features=1.0, random_state=13)\n",
    "        tic = time.perf_counter()\n",
    "        pool.fit(X_train, y_train)\n",
    "        metaclf = KNORAU(pool_classifiers = pool, n_jobs=-1)\n",
    "        metaclf.fit(X_train, y_train)\n",
    "        toc = time.perf_counter()  \n",
    "        training_time += toc-tic \n",
    "\n",
    "        # Evaluate model\n",
    "        tic = time.perf_counter()  \n",
    "        y_hat = metaclf.predict(X_test)\n",
    "        toc = time.perf_counter()  \n",
    "        testing_time += toc-tic    \n",
    "\n",
    "        # Printing a '+' if instance was correctly classified, else prints '-':\n",
    "        if y_hat[0] == y_test.values[0]: print('+', end='')\n",
    "        else: print('-', end='')\n",
    "\n",
    "        # store\n",
    "        y_true.append(y_test.values[0])\n",
    "        y_pred.append(y_hat[0])\n",
    "\n",
    "        summary_of_predictions_dict['Model'].append(model)\n",
    "        summary_of_predictions_dict['Dataset'].append(df.iloc[test_index,:].Dataset.values[0])\n",
    "        summary_of_predictions_dict['Oracle\\'s ST'].append(df.iloc[test_index,:].Best_ST.values[0])\n",
    "        summary_of_predictions_dict['Oracles\\'s Perf'].append(df.iloc[test_index,:].Max_F1_perf.values[0])\n",
    "        summary_of_predictions_dict['Meta-scaler\\'s ST'].append(y_hat[0])\n",
    "        summary_of_predictions_dict['Meta-scaler\\'s Perf'].append(df.iloc[test_index,:][y_hat[0]].values[0])\n",
    "    summary_of_predictions = pd.DataFrame(summary_of_predictions_dict)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    \n",
    "    #print(f\"\\nAccuracy = {acc}\\t\\tF1 = {f1}\")\n",
    "    metamodel_performances_dict['Model'].append(model)\n",
    "    metamodel_performances_dict['Accuracy'].append(acc)\n",
    "    metamodel_performances_dict['F1'].append(f1)\n",
    "\n",
    "metamodel_performances = pd.DataFrame(metamodel_performances_dict)\n",
    "metamodel_performances.to_csv('../results/csv_tabs/metamodel_performances_loocv_KNORAU.csv', encoding='utf8', index=False, float_format='%.4f')\n",
    "\n",
    "print(f'\\nTraining time: {training_time} seconds.')   \n",
    "print(f'Testing time: {testing_time} seconds.')       \n",
    "computing_times['KNORAU'] = {'Testing': testing_time, 'Training': training_time, \n",
    "                            'Total': testing_time+training_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b80b5caf-fbc2-4c3a-aecc-06fab016257b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training metamodel for 'Model' Bagging.\n",
      "--++-+--+-+++----+--+--++-+--+-+-----++++--++--++---+++-+++-+++--++--+--+---++------+----+++--+-+++++-+---+---+--++-++-+++--+----+++++++++++-+++-+-----+-++-+--+-+-----------+--++--++-+--++---+++++++-+-++------+---++-++-+-+------------+------+++-+---+++-+-+---+--------+++--+-+----+------------------+\n",
      "Training metamodel for 'Model' GLVQ.\n",
      "-++--++++++++--+---+-+-+--++-+-++++++++++-+-+---+-+--+-+++++++++-+++++++-++-++-++---++++--+++++++++++-++++++++-++++-++-++-+-+--+++++++++++++++++++++++-++++++++++++++-+-++++--+++++++++-+++++++++++++++++++++++--+++++-+++++++++++++++++++++++++-++++++--++++++++++++++++++-++++++++-++++++++++-+++++++---++\n",
      "Training metamodel for 'Model' GP.\n",
      "-----++++++--++++++++---+----+-+-+++-+++++-+++--++-+-++++++-+++--++--+-+-+-+++-+--+++++++-++--+--++++---++-+-+--++++++-+-+---+++--+++-+--++++-+--+---+-+++++-++++-+----+--------++++++--+-+++-+++++++-++--------+------++--+++-+-++-+++--++---++-+--+++-+++-++++-+++--++-+++--+---++----+-----+++++-+--+--+-\n",
      "Training metamodel for 'Model' KNORAE.\n",
      "--++-++++++-++++++++-----+++-+-+++-+++++-+++-+---+---++-+----+---++----+--+----+-+-+---++-+------++-+---++-+---++-+------+-++-+-+------+---+-------+---++----+--+----+----+++------+----+-------+---+-+----+--+-+------+---+++----++-------------+---+---++--------------+-----++--------++---+++++++-+++-++\n",
      "Training metamodel for 'Model' KNORAU.\n",
      "----+++++++++++-+---+-+++-------+----+++++-++--+---++---+++-+----++--+--+++--+-+-------+++++--+-++--+------+--+-+++--++---+-++--+++++++--+++++++-+-+-+---+-+++-+--+-----+-+--+--+++-++-----+--+++++++--+-++-+---+-----+------------+-++-----+-+--+-+-+--+++--+---+-+-------+++----------++-------+-+-++-----\n",
      "Training metamodel for 'Model' LCA.\n",
      "---+-----+------+++-+----+-----+---------+++--+--+-----++-++++-++++--+--+++--++++----+-----+-+-+--++--++-----+--++-+--+---+-------+--+-+-+---++----++-+-+---+--+------+-+-------++---+--+-+---+-+-----+---------+----------+------+---------+-++-+---+---++--+-+-+-+-----+-------------+-------+--+---------\n",
      "Training metamodel for 'Model' MCB.\n",
      "+-+--+++++++-++-+++++-+--++----++----+++++-+-+++----++-+-++++++--++--+-++++---++-++++-+-----+-++--++-+-+--+----+++-+-+-----+-----++--+--++-+-----++---+++--+---++----+++---++-+---+----++-----+-----+-++--------+---++++-----+----+-+--------+---+---+--++++++---++-----------+--++--+-++++++--++++------++-\n",
      "Training metamodel for 'Model' MLP.\n",
      "+++++++++++-+-+-++++++++++++-++++--+++++++-+--+--+-----++-+-+-+--++++--+--+---+++-----+++++++-+---++++-++++--++++++++++-++--++++--++-----+++----+---+--++++++++++-----------++--++--++---++---+++---+++++++++++--++-+------+++----++---+++++++++-++--++--+++++-+---+--+--++-++---+++---++++---+++++--+-+++++\n",
      "Training metamodel for 'Model' OLA.\n",
      "-----++-++++-++--++++-++-++----+++---+++++-++--+-----++--++-+++---+----+--+----+----+-----+--++---+--------+--+-++++-++----------+--------+-+--++++-+-++-+--+--++-+------+--+-+-++---+-----+-+-------+----+---+++--+-----++----+----+-+-+-++-----+---++-+++-------+-+------+-+---------+-++---+++++----+-++-\n",
      "Training metamodel for 'Model' Percep.\n",
      "-------------++-----------+------+-----+--+-------------+-+-+--++------------+-----------++-+-+-+-----++-----------+--+---++-+--+----------+-++---------+------+----+---+--+--++------+---+------------+---++-+---+----+--++--+--++-----+--------+--++---++----+----+--------++------+++++-+++-+---+-++-+---\n",
      "Training metamodel for 'Model' SVM_RBF.\n",
      "--+--++--++++++++++++----------+++++-+++-+++++-+++-+---++++++++++++--++++++-+-++----+-+-+-+++++++++++-++++++-++++++-++-----+-----+++++++++++---++-+++++++++-+-+++--+--+------++-++++++++++-++--+++++++--+-++-+--+-----++-++-+-+++++-++-+++++++++-+---+---++++++-+++++++--++-----------+++-----++++++-++--++-\n",
      "Training metamodel for 'Model' SVM_lin.\n",
      "-+++++++++++++-++-++-+-+++---+-++----++++-+--++-++---+--+++-+++--++--++--++-++-+-+--+--++-++-+--+++++--+++------+++-++-+--+++--+--++-----+++++-+-+---+-+++--+-+++++++-++++--+---++--+++-----+--+++++++++++++++---+++----+++--+-++-+-+--++++++++++++--+++-++--+-+---++++--++----+----+-+++---+---+--------++-\n",
      "Training time: 280.6951155762654 seconds.\n",
      "Testing time: 27.555637267185375 seconds.\n",
      "CPU times: user 4min 23s, sys: 21.3 s, total: 4min 45s\n",
      "Wall time: 5min 43s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(KNORAU_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68bdb784-d2be-4bde-88fd-8358737ca082",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_ST_results = dataset.set_index(['Model', 'Dataset'])[['NS', 'SS', 'MMS', 'MAS', 'RS', 'QT']] \n",
    "meta_scaler_results = summary_of_predictions.set_index(['Model', 'Dataset'])\n",
    "all_results = pd.concat([static_ST_results, meta_scaler_results], axis = 1).reset_index()\n",
    "all_results.to_csv('../results/csv_tabs/summary_of_predictions_loocv_KNORAU.csv', encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec893db3-e3b2-4010-924a-73a8a6c9546c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Running the experiment with LOOCV. Meta-model: METADES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "612b8cd2-64bf-4032-ac0f-a70ead05411a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture METADES_output\n",
    "%%time\n",
    "\n",
    "training_time = 0  \n",
    "testing_time = 0    \n",
    "# Creating DataFrames to store results:\n",
    "summary_of_predictions_dict = {'Model':[], 'Dataset':[], 'Oracle\\'s ST':[],\n",
    "                               'Oracles\\'s Perf':[], 'Meta-scaler\\'s ST':[], \n",
    "                               'Meta-scaler\\'s Perf':[]}\n",
    "\n",
    "metamodel_performances_dict = {'Model':[], 'Accuracy':[], 'F1': []}\n",
    "\n",
    "models_names = dataset['Model'].unique()\n",
    "\n",
    "#print(f'Shape of original X: {dataset.iloc[:,2:-9].shape}.')\n",
    "for model in models_names: # Running for each of the 12 models\n",
    "    print(f'\\nTraining metamodel for \\'Model\\' {model}.')\n",
    "    df = dataset[dataset['Model'] == model]\n",
    "    df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "    # Separating X and y.\n",
    "    X = df.iloc[:, 2:-9] # Just the metafeatures.\n",
    "    y = df.iloc[:,-1] # Just the best ST.\n",
    "    \n",
    "\n",
    "    # Splitting the dataset into the Training set and Test set folds, according to Leave One Out cross validation:\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    y_true, y_pred = list(), list()\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        #print('.', end='')\n",
    "        # Separating training and test sets:\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # Filling missing values with a KNN imputer.  Each sample’s missing values are imputed\n",
    "        # using the mean value from n_neighbors nearest neighbors found in the training set. \n",
    "        imputer = KNNImputer(n_neighbors=2)\n",
    "        X_train = imputer.fit_transform(X_train)\n",
    "        X_test = imputer.transform(X_test)\n",
    "\n",
    "        # Fitting the metamodel.\n",
    "        # Using class weights to mitigate the effects of class imbalance.\n",
    "        pool = BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight='balanced'),\n",
    "                                 n_jobs=-1, max_samples=0.5, n_estimators=100,\n",
    "                                 bootstrap_features=False, max_features=1.0, random_state=13)\n",
    "        tic = time.perf_counter()\n",
    "        pool.fit(X_train, y_train)\n",
    "        metaclf = METADES(pool_classifiers = pool, n_jobs=-1)\n",
    "        metaclf.fit(X_train, y_train)\n",
    "        toc = time.perf_counter()  \n",
    "        training_time += toc-tic \n",
    "\n",
    "        # Evaluate model\n",
    "        tic = time.perf_counter()  \n",
    "        y_hat = metaclf.predict(X_test)\n",
    "        toc = time.perf_counter()  \n",
    "        testing_time += toc-tic    \n",
    "\n",
    "        # Printing a '+' if instance was correctly classified, else prints '-':\n",
    "        if y_hat[0] == y_test.values[0]: print('+', end='')\n",
    "        else: print('-', end='')\n",
    "\n",
    "        # store\n",
    "        y_true.append(y_test.values[0])\n",
    "        y_pred.append(y_hat[0])\n",
    "\n",
    "        summary_of_predictions_dict['Model'].append(model)\n",
    "        summary_of_predictions_dict['Dataset'].append(df.iloc[test_index,:].Dataset.values[0])\n",
    "        summary_of_predictions_dict['Oracle\\'s ST'].append(df.iloc[test_index,:].Best_ST.values[0])\n",
    "        summary_of_predictions_dict['Oracles\\'s Perf'].append(df.iloc[test_index,:].Max_F1_perf.values[0])\n",
    "        summary_of_predictions_dict['Meta-scaler\\'s ST'].append(y_hat[0])\n",
    "        summary_of_predictions_dict['Meta-scaler\\'s Perf'].append(df.iloc[test_index,:][y_hat[0]].values[0])\n",
    "    summary_of_predictions = pd.DataFrame(summary_of_predictions_dict)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    \n",
    "    #print(f\"\\nAccuracy = {acc}\\t\\tF1 = {f1}\")\n",
    "    metamodel_performances_dict['Model'].append(model)\n",
    "    metamodel_performances_dict['Accuracy'].append(acc)\n",
    "    metamodel_performances_dict['F1'].append(f1)\n",
    "\n",
    "metamodel_performances = pd.DataFrame(metamodel_performances_dict)\n",
    "metamodel_performances.to_csv('../results/csv_tabs/metamodel_performances_loocv_METADES.csv', encoding='utf8', index=False, float_format='%.4f')\n",
    "print(f'\\nTraining time: {training_time} seconds.')   \n",
    "print(f'Testing time: {testing_time} seconds.')       \n",
    "computing_times['METADES'] = {'Testing': testing_time, 'Training': training_time, \n",
    "                            'Total': testing_time+training_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ae42175-0e69-4d0c-98ae-add7a6b4fdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training metamodel for 'Model' Bagging.\n",
      "---+++----+-+----+--+--++-+-++-+-----++++---+--++-----+-+++-+++--++--+--++--+++-+---+-----++----+++++-+---+------++-++---+--+--+-+++++-+++++-+++-+-----+-++-+--+-------+--------++--++-----+---+++++++-+-++------+---++--+-+-----------+--+--+---+++-+---++--+++---+--------+++---------+------------------+\n",
      "Training metamodel for 'Model' GLVQ.\n",
      "-+---+++++++---+++---+-+--++-+-++++++++++-+-+---+-+--+-+++++++++-++-++++-++-++-+------++-++++++++++++-++++++++-++++-+++++-+-++-+++++++++++++++++++-+++-++++++++++++++-+-+-++--+++++++++-+++++++++++++++++++++++-++++++-+++++++++++++++-+++++++++-++++++--++++++++++++++++++-++++++++-++++++++++++++++++---++\n",
      "Training metamodel for 'Model' GP.\n",
      "-----+++++++-++++++++---+----+-+-+-+-+++++-+++--++-+-++++++-+++--++--+-----+++-+--+++++-+-+++-+--++++-+-++-+-+---+++++-+-+---+++--+++-+--++++--------+-+++++-++++-+------------+++++++--+-+-+-+++++++-++---+----++-----+---+++++-++-+++---++--++-++-+++-+++-++++-++++-+--+++------+-----+-----+++++----+--+-\n",
      "Training metamodel for 'Model' KNORAE.\n",
      "--++-++++++-++++++++-----+++-+-++--+++++-+++-+---+--+++-+----+---++-------+----+-------++-+--+---++++---++-+---++-+---+--+-+--+-+------+---+-------+---+-----+--++---+-----+--------------------+---+-+-------+-+----------+++-----+--------++---+---+---++--------------+-----++------++++---+++++-+-+-+-+-\n",
      "Training metamodel for 'Model' KNORAU.\n",
      "-----++++++++++-+-+----+--------+----+++++-++--+---++---+++-+--+-++--+--++++-+-+-------+++++----++--+------+--+++++--++---+-++--+-+++++--+++++++-+-+++-+-+--++-+--------+-+-----+++-++-----+---++++++--+-++-+-+-+------------+-----+-++-----+-+--+---+--+++-++---+-+------++++----+-----++-------+-+-++-----\n",
      "Training metamodel for 'Model' LCA.\n",
      "---+----++-------++-+----+----++---------+++--+--------++-++++-+++---++-+++---++--+--+-----+-+-+--++--++-----+--++-+--+---+-+-----+--+-+-+---++----++---+------+----------------++-+-+-+++++--+-+-----+---------+--+-------+------+-------+-+--+-+---+--+++--+-+-+-+--+--+---+----+----+----------+---------\n",
      "Training metamodel for 'Model' MCB.\n",
      "+-+--++++++++++-+++++-+--+-----++----++++++++--+----++-+-++-++++-++----++++----+-++++-+----++-++--++-+----+----+++-+-+--+--------++--+---+-------++---+++--+---++----++-----+-+---++---+---------------+--------+---+++---++-+----+-+--------+---+-+-+--+++-++---+++----------+------+-+++++----+++------++-\n",
      "Training metamodel for 'Model' MLP.\n",
      "+++++++++++-+-+-++++++++++++-++++-++++++++-++-+--+-----++-++--+--+++---+--+-+--++-----+++++++++---++-+--+++--++++++++++--+---++++-++-+---+++-----+--+--++++++++++-----------++--++-+++---++---+++---+++++++++++--++-+---+--+-+----++---+++++++++-++--++--+++++-+---+--+--++-++---+++---++++-+-+++++--+++++++\n",
      "Training metamodel for 'Model' OLA.\n",
      "----+++--+++--+--++++-+--+++---++----+++++-++--+-----++--++-++++--+----+--+--+-++---+-----+-+-+---+--------+--+-++++-+-----+-----+----------+--++++-+-++-+--+---+-+-----+++-+-+-+----+-----+-+-------+----+---+-+-+++----++----+--------+-+------+---++-+++---------+------------------++++--++++++----+-+++\n",
      "Training metamodel for 'Model' Percep.\n",
      "--------------------------++-----++----+-++-----------------+--++------------------------++-+-+-+------+------------++----++----+--+-------+-++---------+------+----+------+--------+-+----------------+---++-+---+-------++--+--++-----+----+---+--++---++--------------+---++------+++++-+-+-----+-++-----\n",
      "Training metamodel for 'Model' SVM_RBF.\n",
      "-----++--++++++++++++----------+++++-+++-+++++-+++-+---++++++++++++--++++++-+-++----+-+-+-++++++++++++++++++-++-+++-++-----+------++++++++++---+--+++++++++-+-+++-----+------++-+++++++++++-+--+++++++--++++-+--+-----++-++-+-+++++-++-+++++++++-+---+---++++++-+++++++--+++------+---+++-----+++++++++---+-\n",
      "Training metamodel for 'Model' SVM_lin.\n",
      "-+++++++++++++-++-++-+-+++---+-++----++++-+--++-++---+--+++-+++--++--++--++-++-+-+--+---+-++-+--+++++--+++-----++++-++-+--++---+--++-----+++++-+-+---+-+++--+-+++++++-++++--+---++--++------+--+++++++++++++++---+-+----+++--+-++-+-+--+++++++++++++-+++-++--+-+---++++--++----++---+-+++---+---+--------+--\n",
      "Training time: 468.57859996682964 seconds.\n",
      "Testing time: 38.750552125042304 seconds.\n",
      "CPU times: user 7min 15s, sys: 36.6 s, total: 7min 51s\n",
      "Wall time: 9min 2s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(METADES_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5173b4e4-50f3-4bbf-94a5-adcbc7057cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_ST_results = dataset.set_index(['Model', 'Dataset'])[['NS', 'SS', 'MMS', 'MAS', 'RS', 'QT']] \n",
    "meta_scaler_results = summary_of_predictions.set_index(['Model', 'Dataset'])\n",
    "all_results = pd.concat([static_ST_results, meta_scaler_results], axis = 1).reset_index()\n",
    "all_results.to_csv('../results/csv_tabs/summary_of_predictions_loocv_METADES.csv', encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfae666-b746-4782-9d66-537ec3474a0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Running the experiment with LOOCV. Meta-model: DESMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c1c04e3-7506-432f-a736-b7c4e18bbb31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training metamodel for 'Model' Bagging.\n",
      "--++-+--+---+----+--+--+--+----+-----++++--++--++---+++-+++-+++--++--+--+---++--+---+----+++----+++++-+----+--+--++-++-+++--+----++++---++++-+++-+-----+-++-+--+----------------++--++-+--++---++++++--+-+-------++-+++-++-+-++-----+-+---+------+++-----+++-+-+-+-+--------+++----+-----------------------+\n",
      "Training metamodel for 'Model' GLVQ.\n",
      "-++--+++++++---++----+-+----++-++++++++++-+-+---+-+----+++++++++-+++++++-++-++-+-----+++--+++++++++++-++++-+++-++++-+++++---+--+++++++++++++++++++++++-++++++++++++++-+-++++--+++++++++-+++++++++++++++++++++++-++++++-+++++++++++++++++++++++++-++++++--++++++++++++++++++-++++++++-++++++++++-+++++++---++\n",
      "Training metamodel for 'Model' GP.\n",
      "-----++++++--++++++-+---+----+-+-+-+-+++++-+-+--+----++++++-+++--++--+-+-+-+++-+--+++++++-+++-+--++++---++-+-+--++++++-+-+---++---+++-+--+++--+--+-+-+-+++++-++++-+-------------++-+++--+-+++-+++++++--+---+----+-------+--+++-+-++--++--+++--++-+--+--++++-++++-+++--+--+++--+---------+----++++++----+--+-\n",
      "Training metamodel for 'Model' KNORAE.\n",
      "--++-++++++-++++++++-----+++-+-+++-+++++-+++-+------+++-+----+---++-------+----+-+------+-+--+----+-+---++-----++-+---++-+-++-+------------+-----+-----++--+-+--++--------++--------------------+---+-+----+--+-+----+------------++-------------+---+--+++-+-------+----+-----++-------------+++++-+-+++-+-\n",
      "Training metamodel for 'Model' KNORAU.\n",
      "----+++++++++++-+---+-+++-------+----++++++++--+-+-++---+++-+----++--+--+++--+++-------+++++----++--+------+--+-+++--++-+-+-+---+++++++--+++++++-+-+-+---+--++-+--------+-+-----+++-++-----+--+++++++--+-+-++---+-----+----------+-+-++----+--+--+-+-+---++--+-+-+-+-------+++------------------++-+-++-----\n",
      "Training metamodel for 'Model' LCA.\n",
      "---+----+--+----+++-+----+--+--+---------+++--++++-----++-+--+-++-+--+--++++--++-----+-----+-+-+--++--++-----+--++-+--+---+-+-----+-+--+-+---++----++------++--+--------+------+-+------+-----+-+--------------++-+-+---+--+------+-------+-+-++-+---+---++--+---+-+------+------------+++--------+------+--\n",
      "Training metamodel for 'Model' MCB.\n",
      "+-+--++++++--++-+++++-+--+-----++----+++++-+-+++-+---+-+-+++++++-++----++++---++-++++-+--+-++-++--++---+-------+++-+-+--+----+-------+--++-+-----++---+++--+---++----+++---+--+------+-++-----+++------+--------+---++++-----+-----------+---+---+---+--++-++----++--------+--+--++--+-++++-+--++++--+---++-\n",
      "Training metamodel for 'Model' MLP.\n",
      "+++++++++++-+---++++++++++++-++++--+++++++-+--+--+-----++-+-+-+--++++--+--+---+++---+---++++++----++-++++++--+-+++++++---+---+++--++-----+++--+--+-----++++++++++-----------++--++-+++---++---+++---+++++++++++--++-+-----++-+----++---+++++++++-++--++--++-++--+--+-++--++------+++-----++---+++++---+-++++\n",
      "Training metamodel for 'Model' OLA.\n",
      "-----++-++++-++--++-+-++-+--+--++----+++++-++--+---------++-+++---+----++-+----++---+-----+--++---+-------+++-+-++++-+-----------+-+---++-+-+---+++-+-++----+--++-----------+-+-+----+--++-+-+-------+-----++-+-++-+-----++----+----+-+-+-+----+-+---++-+++------++++--+-+++-+---------++++-+-+++++------++-\n",
      "Training metamodel for 'Model' Percep.\n",
      "-+--------+---------+-----+------+--------+-------------+-+-+--++-+-+--------+-----------++---+-+--+---+-+-++--------+----++-+-----++--------+-----------------+--------+--+--++-+----+----------------+-+--+-+---+----+--++--+--+++----+--------+--++---++---------+--+-----+-------+++++-+++-----+-++-+---\n",
      "Training metamodel for 'Model' SVM_RBF.\n",
      "-----++--++++++++++++----+-----+++++-+++-+++++-+++-----++++++++++++--++-+++-+-+++-+----++-+++++++++++-++++++-+-++++-++-----+-----+++++++++++---+--+-++++++--+-+++-----+-++---+++++++++++-++-+--+++++++--+-++-+-++-----++--+-+-+++++-++++++++++++-+---+---++++++-++++++++-+++----------+++-----+++++++++--++-\n",
      "Training metamodel for 'Model' SVM_lin.\n",
      "-+++++++++++++-++-+--+-++----+-++----++++-+--+++-+---+--+++-+++--++--++--+--++---+------+-++----+++++--+++------+++-++----++---+--++-----+++++-+-+-+-+-+++--+++++++++-++++--+---++--+++----++-++++++++++++++++---++++-+-+-+--+-++-+-+-+++++++++++++--+++-++--+-+---++++--+++---+----+-+++------+++-------++-\n",
      "Training time: 284.4762092875317 seconds.\n",
      "Testing time: 29.508998177712783 seconds.\n",
      "CPU times: user 4min 41s, sys: 21.5 s, total: 5min 3s\n",
      "Wall time: 6min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#%%capture DESMI_output\n",
    "\n",
    "training_time = 0\n",
    "testing_time = 0\n",
    "# Creating DataFrames to store results:\n",
    "summary_of_predictions_dict = {'Model':[], 'Dataset':[], 'Oracle\\'s ST':[],\n",
    "                               'Oracles\\'s Perf':[], 'Meta-scaler\\'s ST':[], \n",
    "                               'Meta-scaler\\'s Perf':[]}\n",
    "\n",
    "metamodel_performances_dict = {'Model':[], 'Accuracy':[], 'F1': []}\n",
    "\n",
    "models_names = dataset['Model'].unique()\n",
    "feat_importances = {}\n",
    "\n",
    "for model in models_names: # Running for each of the 12 models\n",
    "#for model in models_names[:1]: # Running first model only  ***** IMPORTANT!!! temporary ****\n",
    "    print(f'\\nTraining metamodel for \\'Model\\' {model}.')\n",
    "    df = dataset[dataset['Model'] == model]\n",
    "    df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "    # Separating X and y.\n",
    "    X = df.iloc[:, 2:-9] # Just the metafeatures.\n",
    "    #X = df.iloc[:50, 2:12] # First 10 metafeatures, first 50 rows. ***** IMPORTANT!!! temporary ****\n",
    "    y = df.iloc[:,-1] # Just the best ST.\n",
    "    \n",
    "    # Creating an array to store the the feature importances for this model:\n",
    "    feat_importances[model] = []\n",
    "\n",
    "    # Splitting the dataset into the Training set and Test set folds, according to Leave One Out cross validation:\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    y_true, y_pred = list(), list()\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        # Separating training and test sets:\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # Filling missing values with a KNN imputer.  Each sample’s missing values are imputed\n",
    "        # using the mean value from n_neighbors nearest neighbors found in the training set. \n",
    "        imputer = KNNImputer(n_neighbors=2)\n",
    "        X_train = imputer.fit_transform(X_train)\n",
    "        X_test = imputer.transform(X_test)\n",
    "\n",
    "        # Fitting the metamodel.\n",
    "        # Using class weights to mitigate the effects of class imbalance.\n",
    "        pool = BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight='balanced'),\n",
    "                                 n_jobs=-1, max_samples=0.5, n_estimators=100,\n",
    "                                 bootstrap_features=False, max_features=1.0, random_state=13)\n",
    "        tic = time.perf_counter()\n",
    "        \n",
    "        pool.fit(X_train, y_train)\n",
    "        metaclf = DESMI(pool_classifiers = pool, n_jobs=-1)\n",
    "        metaclf.fit(X_train, y_train)\n",
    "        \n",
    "        toc = time.perf_counter()\n",
    "        training_time += toc-tic\n",
    "\n",
    "        # Evaluating model\n",
    "        tic = time.perf_counter()\n",
    "        y_hat = metaclf.predict(X_test)\n",
    "        toc = time.perf_counter()\n",
    "        testing_time += toc-tic\n",
    "        \n",
    "        # Printing a '+' if instance was correctly classified, else prints '-':\n",
    "        if y_hat[0] == y_test.values[0]: print('+', end='')\n",
    "        else: print('-', end='')\n",
    "        \n",
    "        # Consulting importances\n",
    "        importances = []\n",
    "        estimators_idxs = metaclf.current_selected_classifiers # This is a matrix w/ the idxs of the estimators used for each instance given to predict().\n",
    "        if estimators_idxs.size == 0: # metaclf did not select classifiers (because all agree):\n",
    "            feat_importances[model].append(pool.estimators_[0].feature_importances_) # then, use the first in the pool\n",
    "        else: #if this list is not empty\n",
    "            estimators = np.array(pool.estimators_)[estimators_idxs[0]] # Since we only gave one instance, we access the first row.\n",
    "            for dt in estimators:\n",
    "                importances.append(dt.feature_importances_)\n",
    "            mean_importances = np.array(pd.DataFrame(importances).mean())\n",
    "            feat_importances[model].append(mean_importances)\n",
    "\n",
    "        # Storing\n",
    "        y_true.append(y_test.values[0])\n",
    "        y_pred.append(y_hat[0])\n",
    "\n",
    "        summary_of_predictions_dict['Model'].append(model)\n",
    "        summary_of_predictions_dict['Dataset'].append(df.iloc[test_index,:].Dataset.values[0])\n",
    "        summary_of_predictions_dict['Oracle\\'s ST'].append(df.iloc[test_index,:].Best_ST.values[0])\n",
    "        summary_of_predictions_dict['Oracles\\'s Perf'].append(df.iloc[test_index,:].Max_F1_perf.values[0])\n",
    "        summary_of_predictions_dict['Meta-scaler\\'s ST'].append(y_hat[0])\n",
    "        summary_of_predictions_dict['Meta-scaler\\'s Perf'].append(df.iloc[test_index,:][y_hat[0]].values[0])\n",
    "    summary_of_predictions = pd.DataFrame(summary_of_predictions_dict)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    \n",
    "    #print(f\"\\nAccuracy = {acc}\\t\\tF1 = {f1}\")\n",
    "    metamodel_performances_dict['Model'].append(model)\n",
    "    metamodel_performances_dict['Accuracy'].append(acc)\n",
    "    metamodel_performances_dict['F1'].append(f1)\n",
    "    feat_importances[model] = pd.DataFrame(feat_importances[model], columns=X.columns).mean()\n",
    "\n",
    "metamodel_performances = pd.DataFrame(metamodel_performances_dict)\n",
    "metamodel_performances.to_csv('../results/csv_tabs/metamodel_performances_loocv_DESMI.csv', encoding='utf8', index=False, float_format='%.4f')\n",
    "pd.DataFrame(feat_importances).to_csv('../results/csv_tabs/feat_importances_DESMI.csv')\n",
    "\n",
    "print(f'\\nTraining time: {training_time} seconds.')\n",
    "print(f'Testing time: {testing_time} seconds.')\n",
    "computing_times['DESMI'] = {'Testing': testing_time, 'Training': training_time, \n",
    "                            'Total': testing_time+training_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0891b4ee-eb68-4791-854b-498df6501037",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_ST_results = dataset.set_index(['Model', 'Dataset'])[['NS', 'SS', 'MMS', 'MAS', 'RS', 'QT']] \n",
    "meta_scaler_results = summary_of_predictions.set_index(['Model', 'Dataset'])\n",
    "all_results = pd.concat([static_ST_results, meta_scaler_results], axis = 1).reset_index()\n",
    "all_results.to_csv('../results/csv_tabs/summary_of_predictions_loocv_DESMI.csv', encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e173dc64-a657-420a-beb8-05680ce257fd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Trying to use SHAP to interpret model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa853fb5-3a5b-457b-9234-f7d154e27d24",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X = df.iloc[:100, 2:12] # First 10 metafeatures, first 100 rows.\n",
    "# y = df.iloc[:100,-1] # Just the best ST.\n",
    "# le = LabelEncoder()\n",
    "# le.fit(y)\n",
    "# new_y = le.transform(y)\n",
    "# new_y = pd.Series(new_y, name='Best_ST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c8f26dd-fef6-4e2a-9845-543d0b38e182",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Creating DataFrames to store results:\n",
    "# summary_of_predictions_dict = {'Model':[], 'Dataset':[], 'Oracle\\'s ST':[],\n",
    "#                                'Oracles\\'s Perf':[], 'Meta-scaler\\'s ST':[], \n",
    "#                                'Meta-scaler\\'s Perf':[]}\n",
    "\n",
    "# metamodel_performances_dict = {'Model':[], 'Accuracy':[], 'F1': []}\n",
    "\n",
    "# models_names = dataset['Model'].unique()\n",
    "\n",
    "# SHAP_values_per_model = {}\n",
    "# test_indices_per_model = {}\n",
    "\n",
    "# #print(f'Shape of original X: {dataset.iloc[:,2:-9].shape}.')\n",
    "# for model in models_names: # Running for each of the 12 models\n",
    "#     print(f'\\nTraining metamodel for \\'Model\\' {model}.')\n",
    "#     df = dataset[dataset['Model'] == model]\n",
    "#     df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "#     # Separating X and y.\n",
    "#     #X = df.iloc[:, 2:-9] # Just the metafeatures.\n",
    "#     #y = df.iloc[:,-1] # Just the best ST.\n",
    "#     X = df.iloc[:100, 2:12] # First 10 metafeatures, first 100 rows. ***** IMPORTANT!!! temporary ****\n",
    "#     y = df.iloc[:100,-1] # Just the best ST.\n",
    "\n",
    "#     le = LabelEncoder()\n",
    "#     le.fit(y)\n",
    "#     y = le.transform(y)\n",
    "#     y = pd.Series(y, name='Best_ST')\n",
    "   \n",
    "#     # Splitting the dataset into the Training set and Test set folds, according to Leave One Out cross validation:\n",
    "#     loo = LeaveOneOut()\n",
    "    \n",
    "#     test_indices = []\n",
    "#     SHAP_values_per_fold = []\n",
    "    \n",
    "#     y_true, y_pred = list(), list()\n",
    "#     for train_index, test_index in loo.split(X):\n",
    "#         #print('.', end='')\n",
    "#         # Separating training and test sets:\n",
    "#         X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "#         y_train, y_test = y[train_index], y[test_index]\n",
    "       \n",
    "#         test_indices.append(test_index) # Storing it for reindexing the X according to SHAP values later.\n",
    "#         # Filling missing values with a KNN imputer.  Each sample’s missing values are imputed\n",
    "#         # using the mean value from n_neighbors nearest neighbors found in the training set. \n",
    "#         imputer = KNNImputer(n_neighbors=2)\n",
    "#         X_train = imputer.fit_transform(X_train)\n",
    "#         X_test = imputer.transform(X_test)\n",
    "        \n",
    "#         # Fitting the metamodel.\n",
    "#         # Using class weights to mitigate the effects of class imbalance.\n",
    "#         pool = BaggingClassifier(estimator=DecisionTreeClassifier(class_weight='balanced'),\n",
    "#                                  n_jobs=-1, max_samples=0.5, n_estimators=100,\n",
    "#                                  bootstrap_features=False, max_features=1.0, random_state=13)\n",
    "#         pool.fit(X_train, y_train)\n",
    "#         metaclf = DESMI(pool_classifiers = pool, n_jobs=-1)\n",
    "#         metaclf.fit(X_train, y_train)\n",
    "        \n",
    "#         # Evaluate model\n",
    "#         y_hat = metaclf.predict(X_test)\n",
    "\n",
    "#         # Printing a '+' if instance was correctly classified, else prints '-':\n",
    "#         if y_hat[0] == y_test.values[0]: print('+', end='')\n",
    "#         else: print('-', end='')\n",
    "        \n",
    "#         # store\n",
    "#         y_true.append(y_test.values[0])\n",
    "#         y_pred.append(y_hat[0])\n",
    "\n",
    "#         summary_of_predictions_dict['Model'].append(model)\n",
    "#         summary_of_predictions_dict['Dataset'].append(df.iloc[test_index,:].Dataset.values[0])\n",
    "#         summary_of_predictions_dict['Oracle\\'s ST'].append(df.iloc[test_index,:].Best_ST.values[0])\n",
    "#         summary_of_predictions_dict['Oracles\\'s Perf'].append(df.iloc[test_index,:].Max_F1_perf.values[0])\n",
    "#         summary_of_predictions_dict['Meta-scaler\\'s ST'].append(y_hat[0])\n",
    "#         # print(f'y_hat[0] = ',y_hat[0])\n",
    "#         # print(f'le.inverse_transform([y_hat[0]]) = ', le.inverse_transform([y_hat[0]]))\n",
    "        \n",
    "#         #summary_of_predictions_dict['Meta-scaler\\'s Perf'].append(df.iloc[test_index,:][y_hat[0]].values[0])\n",
    "#         summary_of_predictions_dict['Meta-scaler\\'s Perf'].append(df.iloc[test_index,:][le.inverse_transform([y_hat[0]])[0]].values[0])\n",
    "#         # Explain predictions for this meta-classifier (trained in this fold) and store them\n",
    "        \n",
    "#         explainer = shap.Explainer(metaclf.predict_proba, X_test, \n",
    "#                                    output_names=y.unique(), \n",
    "#                                    algorithm='permutation')\n",
    "#         shap_values = explainer(X_test)\n",
    "#         print(len(shap_values))\n",
    "#         print(shap_values)  # Why are these all zeroes???\n",
    "        \n",
    "#         for SHAPs in shap_values:\n",
    "#             SHAP_values_per_fold.append(SHAPs) \n",
    "        \n",
    "#     # SHAP_values_per_model[model] = SHAP_values_per_fold\n",
    "#     # test_indices_per_model[model] = test_indices\n",
    "    \n",
    "    \n",
    "\n",
    "#     # Transposition trick to correct the shape of SHAP_values:\n",
    "#     SHAP_values=[]\n",
    "#     for t in SHAP_values_per_fold:\n",
    "#         SHAP_values.append(t.values)\n",
    "#     SHAP_values = np.array(SHAP_values).transpose(2,0,1)\n",
    "#     #print(SHAP_values.shape)\n",
    "#     SHAP_values = list(SHAP_values)\n",
    "#     # SHAP plots:\n",
    "#     test_indcs = test_indices\n",
    "#     new_index = list(pd.concat([pd.Series(v) for v in test_indcs]).values)\n",
    "#     for i in range(6):\n",
    "#         print(f'Summary plot for Class {i}:')\n",
    "#         shap.summary_plot(SHAP_values[i], X.reindex(new_index), use_log_scale=False, show=False)\n",
    "#         plt.savefig(f'../figs/shap_summary_plots_model_{model}_Class_{i}.pdf', bbox_inches = 'tight')\n",
    "#         plt.close()\n",
    "#     shap.summary_plot(SHAP_values, X.reindex(new_index), plot_type='bar', show=False)\n",
    "#     plt.savefig(f'../figs/shap_bar_plots_model_{model}.pdf', bbox_inches = 'tight')\n",
    "#     plt.close()\n",
    "    \n",
    "    \n",
    "#     summary_of_predictions = pd.DataFrame(summary_of_predictions_dict)\n",
    "#     acc = accuracy_score(y_true, y_pred)\n",
    "#     f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    \n",
    "#     #print(f\"\\nAccuracy = {acc}\\t\\tF1 = {f1}\")\n",
    "#     metamodel_performances_dict['Model'].append(model)\n",
    "#     metamodel_performances_dict['Accuracy'].append(acc)\n",
    "#     metamodel_performances_dict['F1'].append(f1)\n",
    "\n",
    "# metamodel_performances = pd.DataFrame(metamodel_performances_dict)\n",
    "# metamodel_performances.to_csv('../results/csv_tabs/metamodel_performances_loocv_DESMI_new.csv', encoding='utf8', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80e15e8e-e48e-400d-8b13-eef4255282ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #model = 'Bagging'\n",
    "# for model in models_names[:1]:\n",
    "#     # Transposition trick to correct the shape of SHAP_values:\n",
    "#     SHAP_values=[]\n",
    "#     for t in SHAP_values_per_model[model]:\n",
    "#         SHAP_values.append(t.values)\n",
    "#     SHAP_values = np.array(SHAP_values).transpose(2,0,1)\n",
    "#     #print(SHAP_values.shape)\n",
    "#     SHAP_values = list(SHAP_values)\n",
    "#     # SHAP plots:\n",
    "#     test_indcs = test_indices_per_model[model]\n",
    "#     new_index = list(pd.concat([pd.Series(v) for v in test_indcs]).values)\n",
    "#     for i in range(6):\n",
    "#         print(f'Summary plot for Class {i}:')\n",
    "#         shap.summary_plot(SHAP_values[i], X.reindex(new_index), use_log_scale=True, show=False)\n",
    "#         plt.savefig(f'../figs/shap_summary_plots_model_{model}_Class_{i}.pdf', bbox_inches = 'tight')\n",
    "#         plt.close()\n",
    "#     shap.summary_plot(SHAP_values, X.reindex(new_index), plot_type='bar')\n",
    "#     plt.savefig(f'../figs/shap_bar_plots_model_{model}.pdf', bbox_inches = 'tight')\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "402fccb6-2f14-42bd-9891-4c0224aa4a98",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# static_ST_results = dataset.set_index(['Model', 'Dataset'])[['NS', 'SS', 'MMS', 'MAS', 'RS', 'QT']] \n",
    "# meta_scaler_results = summary_of_predictions.set_index(['Model', 'Dataset'])\n",
    "# all_results = pd.concat([static_ST_results, meta_scaler_results], axis = 1).reset_index()\n",
    "# all_results.to_csv('../results/csv_tabs/summary_of_predictions_loocv_DESMI_new.csv', encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e1d40b-5f50-49ed-a06f-1e8fcabf6123",
   "metadata": {},
   "source": [
    "# Saving computing Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd615ef3-99f3-49a6-8972-b7079415d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(computing_times).to_csv('../results/csv_tabs/computing_times.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d47708-7062-461a-9c59-f190a5162eee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# (Deprecated) Running the experiment with 5-fold CV. Meta-model: RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d562e613-3346-418f-b5a1-aba5fd364296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Creating DataFrame (initially a dict) to store base model results, one row per fold.\n",
    "# summary_of_predictions_dict = {'Model':[], 'Meta-scaler\\'s STs':[], \n",
    "#                                'Meta-scaler\\'s mean Perf':[]}\n",
    "\n",
    "# metamodel_performances_dict = {'Model':[], 'Accuracy':[], 'F1': []}\n",
    "\n",
    "# models_names = dataset['Model'].unique()\n",
    "\n",
    "# #print(f'Shape of original X: {dataset.iloc[:,2:-9].shape}.')\n",
    "# for model in models_names: # Running for each of the 11 models\n",
    "#     print(f'\\nTraining metamodel for \\'Model\\' {model}.')\n",
    "#     df = dataset[dataset['Model'] == model]\n",
    "#     df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "#     # Separating X and y.\n",
    "#     X = df.iloc[:, 2:-9] # Just the metafeatures.\n",
    "#     y = df.iloc[:,-1] # Just the best ST.\n",
    "    \n",
    "\n",
    "#     # Splitting the dataset into the Training set and Test set folds, \n",
    "#     # according to 5-fold cross validation:\n",
    "#     cv = StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    \n",
    "#     ## This fails because one needs to do the imputation first (remove NaNs):\n",
    "#     # meta_model = RandomForestClassifier(class_weight='balanced')\n",
    "#     # cv_results = cross_validate(meta_model, X, y, cv=5, scoring= ['accuracy', 'f1_macro'])\n",
    "#     acc_folds = []\n",
    "#     f1_folds = []\n",
    "#     y_true, y_pred = list(), list()\n",
    "#     for train_index, test_index in cv.split(X, y):\n",
    "#         print('.', end='')\n",
    "#         # Separating training and test sets:\n",
    "#         X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "#         y_train, y_test = y[train_index], y[test_index]\n",
    "#         # Filling missing values with a KNN imputer.  Each sample’s missing values are imputed\n",
    "#         # using the mean value from n_neighbors nearest neighbors found in the training set. \n",
    "#         imputer = KNNImputer(n_neighbors=2)\n",
    "#         X_train = imputer.fit_transform(X_train)\n",
    "#         X_test = imputer.transform(X_test)\n",
    "        \n",
    "#         # Fitting the metamodel.\n",
    "#         # Using class weights to mitigate the effects of class imbalance.\n",
    "#         #metaclf = SVC(class_weight='balanced')\n",
    "#         metaclf = RandomForestClassifier(class_weight='balanced')\n",
    "#         metaclf.fit(X_train, y_train)\n",
    "\n",
    "#         # Evaluate model\n",
    "#         y_pred = metaclf.predict(X_test)\n",
    "\n",
    "#         # Calculate metrics\n",
    "#         y_true = y_test\n",
    "#         acc_folds.append(accuracy_score(y_true, y_pred))\n",
    "#         f1_folds.append(f1_score(y_true, y_pred, average=\"macro\"))\n",
    "        \n",
    "#         #print(y_test.values, y_pred)\n",
    "#         #print(f'F1 = {f1_score(y_test, y_pred, average=\"weighted\")}  \\taccuracy = {accuracy_score(y_test,y_pred)}')\n",
    "#         summary_of_predictions_dict['Model'].append(model)\n",
    "#         # summary_of_predictions_dict['Oracle\\'s STs'].append(df.iloc[test_index,:].Best_ST.values)\n",
    "#         # summary_of_predictions_dict['Oracles\\'s mean Perf'].append(df.iloc[test_index,:].Max_F1_perf.mean())\n",
    "#         summary_of_predictions_dict['Meta-scaler\\'s STs'].append(y_pred)\n",
    "#         d = list(zip(test_index, y_pred))\n",
    "#         ms_perfs = []\n",
    "#         for (idx, st) in d: ms_perfs.append(df.iloc[idx,:][st])\n",
    "#         summary_of_predictions_dict['Meta-scaler\\'s mean Perf'].append(np.mean(ms_perfs))\n",
    "#     summary_of_predictions = pd.DataFrame(summary_of_predictions_dict)\n",
    "    \n",
    "#     #print(f\"\\nAccuracy = {acc}\\t\\tF1 = {f1}\")\n",
    "#     metamodel_performances_dict['Model'].append(model)\n",
    "#     metamodel_performances_dict['Accuracy'].append(np.mean(acc_folds))\n",
    "#     metamodel_performances_dict['F1'].append(np.mean(f1_folds))\n",
    "\n",
    "# metamodel_performances = pd.DataFrame(metamodel_performances_dict)\n",
    "# metamodel_performances.to_csv('../results/csv_tabs/metamodel_performances_5foldcv_RF.csv', encoding='utf8', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf1e22eb-000f-42fd-8e2e-234ddd01ec9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# static_ST_results = dataset[['Model', 'NS', 'SS', 'MMS', 'MAS', 'RS', 'QT', 'Max_F1_perf']].groupby(['Model']).mean()\n",
    "# static_ST_results = static_ST_results.rename(columns={'Max_F1_perf':'Oracle'})\n",
    "# summary_of_predictions_by_model = summary_of_predictions.groupby(['Model']).mean()\n",
    "# all_results = pd.concat([static_ST_results,summary_of_predictions_by_model], axis = 1)\n",
    "# all_results = all_results.reset_index()\n",
    "# all_results.to_csv('../results/csv_tabs/summary_classification_performances_5foldCV_RF.csv', encoding='utf8', index=False, float_format='%.4f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
